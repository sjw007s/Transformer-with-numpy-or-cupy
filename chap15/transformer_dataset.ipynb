{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap05/dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-da80162dc5ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mopenpyxl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_workbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mtransformer_Dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer_Dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NLP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "class transformer_Dataset(Dataset):\n",
    "    def __init__(self, filenames, timesteps=10):\n",
    "        super(transformer_Dataset, self).__init__('NLP', 'binary')\n",
    "        self.training_target_corpus=[] #\n",
    "        self.training_original_corpus=[]  #index\n",
    "        self.original_counting_corpus = {} #\n",
    "        self.counting_list=[] #\n",
    "        self.target_word_to_id = {} #\n",
    "        self.target_id_to_word = {} #\n",
    "        self.original_word_to_id = {}  #\n",
    "        self.original_id_to_word = {}  #\n",
    "        self.training_original_one_hot_word = 0   #\n",
    "        self.training_target_one_hot_word=0 #\n",
    "        self.original_unique_words = 0 #\n",
    "        self.negative=[]\n",
    "        self.most_length=0      #\n",
    "        self.target_sentence_to_id = {}\n",
    "        self.target_id_to_sentence = {}  #\n",
    "        self.original_sentence_to_id = {}  \n",
    "        self.original_id_to_sentence = {}  #\n",
    "        self.training_one_hot_sentence = 0\n",
    "        self.original_unique_sentence = 0 #\n",
    "        self.original_text=[]  #raw\n",
    "        self.target_text=[]   #\n",
    "        self.original_token_text=[]   #\n",
    "        self.Max_data=9    #\n",
    "        self.most_length_target=0\n",
    "        \n",
    "    def text_find_number_words(self,name, training):\n",
    "        full_text=list()\n",
    "        name='text_data/'+name\n",
    "        load_wb = load_workbook(name, data_only=True)\n",
    "        load_ws = load_wb['Sheet1']\n",
    "        #print(load_ws.rows)\n",
    "        for i, row in enumerate(load_ws.iter_rows(min_row=1, max_row=1309, min_col=1, max_col=20000, values_only=True)):\n",
    "            #print(i)\n",
    "            if i ==self.Max_data:\n",
    "                break\n",
    "            temp_list=list()\n",
    "            for j, cell in enumerate(row):\n",
    "                if j == 0:\n",
    "                    self.original_text.append(str(cell).lower())\n",
    "                    #print(cell.lower())\n",
    "                    continue\n",
    "                if cell !=None:\n",
    "                    temp_list.append(str(cell).lower())\n",
    "       \n",
    "                    pass\n",
    "                else:\n",
    "                    for word in temp_list:\n",
    "                        if word not in self.target_word_to_id:\n",
    "                            new_id = len(self.target_word_to_id)\n",
    "                            self.target_word_to_id[word] = new_id\n",
    "                            self.target_id_to_word[new_id] = word\n",
    "                    if self.most_length_target<len(temp_list):\n",
    "                        self.most_length_target=len(temp_list)\n",
    "                    \n",
    "                    self.training_target_corpus.append([self.target_word_to_id[w] for w in temp_list])\n",
    "                    self.target_id_to_sentence[i] = temp_list\n",
    "                    self.target_text.append(' '.join(temp_list))\n",
    "                  \n",
    "                    break\n",
    "        #self.most_length_target=1\n",
    "        self.most_length_target+=1\n",
    "        self.target_word_to_id['sep0'] = len(self.target_word_to_id)\n",
    "        self.target_id_to_word[len(self.target_word_to_id)-1]='sep0'\n",
    "        self.target_word_to_id['start0'] = len(self.target_word_to_id)\n",
    "        self.target_id_to_word[len(self.target_word_to_id)-1]='start0'\n",
    "        #print(self.target_id_to_word)\n",
    "        for i in range(self.Max_data):\n",
    "            self.training_target_corpus[i]=[len(self.target_word_to_id)-1]+self.training_target_corpus[i]\n",
    "        #print(self.training_target_corpus)\n",
    "        for i, row_origin in enumerate(self.original_text):\n",
    "            words_origin = row_origin.lower().replace('.',' .').replace(',',' ,').split(' ')\n",
    "            self.original_token_text.append(words_origin)\n",
    "            if self.most_length<len(words_origin):\n",
    "                self.most_length=len(words_origin)\n",
    "            for word in words_origin:\n",
    "                if word not in self.original_word_to_id:\n",
    "                    new_id = len(self.original_word_to_id)\n",
    "                    self.original_word_to_id[word] = new_id\n",
    "                    self.original_id_to_word[new_id] = word\n",
    "                \n",
    "            self.training_original_corpus.append([self.original_word_to_id[w] for w in words_origin])\n",
    "            self.original_id_to_sentence[i] = row_origin\n",
    "        self.training_original_corpus_numpy=np.zeros((self.Max_data,self.most_length), dtype='int32')\n",
    "        self.training_target_corpus_numpy=np.zeros((self.Max_data,self.most_length_target), dtype='int32')\n",
    "        for i in range(self.Max_data):\n",
    "            for j in range(self.most_length):\n",
    "                try:\n",
    "                    self.training_original_corpus_numpy[i,j]=self.training_original_corpus[i][j]\n",
    "                except:\n",
    "                    self.training_original_corpus_numpy[i,j]=len(self.original_word_to_id)\n",
    "        for i in range(self.Max_data):\n",
    "            for j in range(self.most_length_target):\n",
    "                try:\n",
    "                    self.training_target_corpus_numpy[i,j]=self.training_target_corpus[i][j]\n",
    "                except:\n",
    "                    self.training_target_corpus_numpy[i,j]=len(self.target_word_to_id)-2\n",
    "        \n",
    "        if training==1:\n",
    "            self.training_original_one_hot_word = np.eye(len(self.original_word_to_id),dtype = 'float64')\n",
    "            self.training_target_one_hot_word = np.eye(len(self.target_word_to_id),dtype = 'float64')\n",
    "            self.original_unique_words = len(self.original_word_to_id)\n",
    "            self.target_unique_words = len(self.target_word_to_id)\n",
    "            #print(\"dd\",self.target_word_to_id)\n",
    "            self.original_unique_sentence = len(self.original_id_to_sentence)\n",
    "            \"\"\"\n",
    "            sum_=0\n",
    "            for i in range(self.original_unique_sentence):\n",
    "                for j in self.training_original_corpus[i]:\n",
    "                    if j not in self.original_counting_corpus:\n",
    "                        self.original_counting_corpus[j] = 1\n",
    "                        sum_+=1\n",
    "                    else:\n",
    "                        self.original_counting_corpus[j]+=1\n",
    "                        sum_+=1\n",
    "            for i in range(len(self.original_counting_corpus)):\n",
    "                self.counting_list.append(self.original_counting_corpus[i]/sum_)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            pass\n",
    "        #self.training_original_corpus=np.array(self.training_original_corpus)\n",
    "        #self.training_target_corpus=np.array(self.training_target_corpus)\n",
    "        print(\"input most_length\",self.most_length)\n",
    "    @property\n",
    "    def train_count(self):\n",
    "        return len(self.training_original_corpus)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '{}({}, {} train_data)'.format(self.name, self.mode, self.train_count)\n",
    "    \n",
    "    def get_train_data(self, batch_size, nth):\n",
    "        from_idx = nth * batch_size\n",
    "        to_idx = (nth + 1) * batch_size\n",
    "    \n",
    "\n",
    "        tr_X = self.training_original_corpus_numpy[self.indices[from_idx:to_idx]]\n",
    "        tr_Y = self.training_target_corpus_numpy[self.indices[from_idx:to_idx]]\n",
    "        \n",
    "        return tr_X, tr_Y\n",
    "\n",
    "    def shuffle_train_data(self, size):\n",
    "        np.random.seed(int(time.time()))\n",
    "        self.indices = np.arange(size)\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "\n",
    "\n",
    "      \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
