{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap15/gan.ipynb\n",
    "%run ../chap15/NLP_dataset.ipynb\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP(Gan):\n",
    "    def __init__(self, name, dataset, hconfigs, JJ=0, show_maps=False,\n",
    "                 l2_decay=0, l1_decay=0, dump_structure=True, word_vector_dimension=100, window_size=1, negative=5, load=0):\n",
    "        self.word_vector_dimension=word_vector_dimension\n",
    "        self.training_unique_words_number=0\n",
    "        self.training_full_text_number=0\n",
    "        self.new_words_number=0\n",
    "        self.new_full_text_number=0\n",
    "        self.window_size=window_size\n",
    "        self.negative = negative\n",
    "        self.text_dataset  = NLPDataset(['initial'])\n",
    "        self.text_dataset.text_find_number_words(\"training_text\",1)\n",
    "        self.text_dataset.create_contexts_target(self.text_dataset.training_corpus,self.window_size)\n",
    "        self.load=load\n",
    "        super(NLP, self).__init__(name, dataset, hconfigs, show_maps,\n",
    "                                          l2_decay, l1_decay)\n",
    "        \n",
    "    def train(self,epoch_count, batch_size, learning_rate, learning_decrease,restore, name):\n",
    "        print(\"the number of words and the number of unique words\")\n",
    "        print(len(self.text_dataset.training_corpus),self.text_dataset.unique_words)\n",
    "        self.training_full_text_number=len(self.text_dataset.training_corpus)\n",
    "        self.training_unique_words_number=self.text_dataset.unique_words\n",
    "        \n",
    "        if self.load == 1:\n",
    "            self.load_parameters(\"dic\")\n",
    "  \n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n=0\n",
    "        self.batch_size=batch_size\n",
    "        batch_count = int(self.text_dataset.train_count / batch_size)\n",
    "        time1 = time2 = int(time.time())\n",
    "  \n",
    "        for epoch in range(epoch_count):\n",
    "            costs = []\n",
    "            accs = []\n",
    "            self.epoch = epoch\n",
    "            self.learning_rate = self.learning_rate*learning_decrease\n",
    "            self.text_dataset.shuffle_train_data(batch_size*batch_count)\n",
    "  \n",
    "            for n in range(batch_count):\n",
    "                self.n=n\n",
    "                trX, trY = self.text_dataset.get_train_data(batch_size, n)\n",
    "                cost= self.train_step(trX, trY)\n",
    "                \n",
    "                with open(restore,'a',newline='') as aa:\n",
    "                    writer = csv.writer(aa, delimiter=',')\n",
    "                    time3 = int(time.time())\n",
    "                    tm1, tm2 = time3-time2, time3-time1 #tm1은 한 에포크 시간, tm2는 첨부터 시간\n",
    "                    time2 = time3\n",
    "                    writer.writerow([epoch+1]+[n]+[self.test()])\n",
    "                    \n",
    "            if epoch % 10==0 and epoch != 0:\n",
    "                self.save_parameters(\"dic\")\n",
    "  \n",
    "            \n",
    "        tm_total = int(time.time()) - time1\n",
    "        print('Model {} train ended in {} secs:'.format(self.name, tm_total))\n",
    "    \n",
    "    def init_parameters(self, hconfigs):\n",
    "        self.hconfigs = hconfigs\n",
    "        self.pm_hiddens = []\n",
    "        \n",
    "        pm_hidden = self.alloc_embedding_layer(len(self.text_dataset.word_to_id),self.word_vector_dimension) #가중치 어휘수 * 벡터 차원 \n",
    "        prev_shape = self.word_vector_dimension*2*self.window_size\n",
    "        self.pm_hiddens.append(pm_hidden)\n",
    "        \n",
    "        for hconfig in hconfigs:\n",
    "            pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig)\n",
    "            self.pm_hiddens.append(pm_hidden)\n",
    "            \n",
    "        pm_hidden = self.alloc_end_embedding_layer(prev_shape)\n",
    "        self.pm_hiddens.append(pm_hidden)\n",
    "        \n",
    "        \n",
    "    def train_step(self, x, y):\n",
    "        self.is_training = True\n",
    "        loss, aux_nn = self.forward_neuralnet(x,y)\n",
    "        G_loss = 1.0\n",
    "        self.backprop_neuralnet(G_loss, aux_nn)\n",
    "        self.is_training = False\n",
    "        return loss\n",
    "    \n",
    "    def forward_neuralnet(self, x,y):\n",
    "        hidden = x\n",
    "        aux_layers = []\n",
    "\n",
    "        hidden, aux = self.forward_embedding_layer(hidden, None, self.pm_hiddens[0])\n",
    "        aux_layers.append(aux)\n",
    "        \n",
    "        for n, hconfig in enumerate(self.hconfigs):\n",
    "            hidden, aux = self.forward_layer(hidden, hconfig, self.pm_hiddens[n+1])\n",
    "            aux_layers.append(aux)\n",
    "\n",
    "        output, aux = self.forward_end_embedding_layer(hidden, self.pm_hiddens[-1],y)\n",
    "\n",
    "        return output, [aux, aux_layers]\n",
    "\n",
    "    def backprop_neuralnet(self, G_output, aux):\n",
    "        aux_out, aux_layers = aux\n",
    "    \n",
    "        G_hidden = self.backprop_end_embedding_layer(G_output, self.pm_hiddens[-1], aux_out)\n",
    "    \n",
    "\n",
    "        for n in reversed(range(len(self.hconfigs))):\n",
    "            hconfig, pm, aux = self.hconfigs[n], self.pm_hiddens[n+1], aux_layers[n+1]\n",
    "            G_hidden = self.backprop_layer(G_hidden, hconfig, pm, aux)\n",
    "        \n",
    "        hconfig, pm, aux = self.hconfigs[0], self.pm_hiddens[0], aux_layers[0]\n",
    "        G_hidden = self.backprop_embedding_layer(G_hidden, hconfig, pm, aux)\n",
    "        \n",
    "        return G_hidden\n",
    "      \n",
    "    def test(self,a='is', b='are'):\n",
    "        a_=self.text_dataset.word_to_id[a]\n",
    "        b_=self.text_dataset.word_to_id[b]\n",
    "\n",
    "        print(np.dot(self.pm_hiddens[0]['dic'][a_], self.pm_hiddens[0]['dic'][b_])/((sum(self.pm_hiddens[0]['dic'][a_]*self.pm_hiddens[0]['dic'][a_])\\\n",
    "                                                                                    **0.5)*(sum(self.pm_hiddens[0]['dic'][b_]*self.pm_hiddens[0]['dic'][b_])**0.5)))\n",
    "        \n",
    "    def predict_word(self,word_1):\n",
    "        pass\n",
    "    \n",
    "    def alloc_embedding_layer(self, input_shape, hconfig):\n",
    "        input_cnt = np_cpu.prod(input_shape)\n",
    "        output_cnt = get_conf_param(hconfig, 'width', hconfig)\n",
    "        weight = np.random.normal(0, self.rand_std, [input_cnt, output_cnt], dtype = 'float32')#가중치 (어휘수 * 벡터 차원)\n",
    "        return {'dic':weight}\n",
    "\n",
    "    def forward_embedding_layer(self, x, hconfig, pm):\n",
    "        #print(pm['dic'][x,:].shape)\n",
    "        return pm['dic'][x,:].reshape(self.batch_size,-1), [x, pm['dic'][x,:].reshape(self.batch_size,-1)]\n",
    "\n",
    "    def backprop_embedding_layer(self, G_y, hconfig, pm, aux):\n",
    "        if pm is None: return G_y\n",
    "        x, y= aux\n",
    "        g_affine_weight = self.text_dataset.training_one_hot_word[x].transpose()\n",
    "        for i in range(2*self.window_size):\n",
    "            #print(\"Asdfasdfasdfasd\",g_affine_weight.shape,G_y.shape)\n",
    "            G_weight = np.matmul(g_affine_weight[:,i,:], G_y[:,i*self.word_vector_dimension:(i+1)*self.word_vector_dimension])\n",
    "            self.update_param(pm, 'dic', G_weight)\n",
    "        return 0\n",
    "    \n",
    "    def alloc_end_embedding_layer(self, input_shape):\n",
    "        input_cnt = np_cpu.prod(input_shape)\n",
    "        weight = np.random.normal(0, self.rand_std, [input_cnt, len(self.text_dataset.word_to_id)], dtype = 'float32')#가중치 (히든레이어 * 벡터 차원)\n",
    "        return {'dic_end':weight}\n",
    "\n",
    "    def forward_end_embedding_layer(self, x, pm,y):\n",
    "        temp_number_list=list()\n",
    "        for i in range(self.batch_size):\n",
    "            while 1==1:\n",
    "                temp_number_=list(map(int, np.random.choice(self.text_dataset.np_arange_, size=self.negative,p=self.text_dataset.counting_list).tolist()))\n",
    "                if y[i] in temp_number_:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_number_list.append(temp_number_)\n",
    "                    break\n",
    "\n",
    "        output_list=list()\n",
    "        y_one_list=list()\n",
    "        self.temp_pm_false=list()\n",
    "        #print(\"종우\", y.shape, len(temp_number_list[0]))\n",
    "        for i in range(self.negative+1):\n",
    "            \n",
    "            if i ==0:\n",
    "                output = np.matmul(x, pm['dic_end'][:,np.array(y,dtype='int32').tolist()])\n",
    "                self.temp_pm_false.append(y)\n",
    "            else:\n",
    "                output = np.matmul(x, pm['dic_end'][:,np.array(temp_number_list, dtype='int32')[:,i-1].tolist()])\n",
    "                self.temp_pm_false.append(np.array(temp_number_list, dtype='int32')[:,i-1].tolist())\n",
    "            #print(x.shape,pm['dic_end'][:,np.array(y,dtype='int32').tolist()].shape)\n",
    "            #output = np.sum(output,axis=1) #output (2500,300) -> 2500,1\n",
    "            output = self.activate(output, {'actfunc':'sigmoid'})#3\n",
    "            if i == 0:\n",
    "                y_one=np.ones(output.shape,dtype='float32')\n",
    "            else:\n",
    "                y_one=np.zeros(output.shape,dtype='float32')\n",
    "            entropy = sigmoid_cross_entropy_with_logits(y_one, output)#2 (2500,1)\n",
    "            loss = np.mean(entropy)#1\n",
    "            print('loss:',loss)\n",
    "            output_list.append(output)\n",
    "            y_one_list.append(y_one)\n",
    "            \"\"\"\n",
    "            for j in range(self.batch_size):\n",
    "                print(\"y\",y.shape,pm['dic_end'].shape)\n",
    "                print(i,j,x.shape,pm['dic_end'][:,np.array(y[j],dtype='int32').tolist()].shape)\n",
    "                output = np.matmul(x,pm['dic_end'][:,np.array(y[j],dtype='int32').tolist()])\n",
    "                output = self.activate(output, {'actfunc':'sigmoid'})\n",
    "                print(\"asdfasdf\",output.shape)\n",
    "                output_list.append(output)\n",
    "            \n",
    "                    \n",
    "            \"\"\"    \n",
    "                \n",
    "        aux = [y_one_list, output_list, x, x.shape,temp_number_list,output.shape,y]\n",
    " \n",
    "        return loss, aux\n",
    "\n",
    "    def backprop_end_embedding_layer(self, G_loss, pm, aux):\n",
    "        if pm is None: return G_y\n",
    "        \n",
    "        y_one_list, output_list, x,x_org_shape,temp_number_list,output_shape, y= aux\n",
    "        shape = output_list[-1].shape\n",
    "        #print(shape)\n",
    "        for i in reversed(range(self.negative+1)):\n",
    "            self.i_i_i=i\n",
    "            #print(i)\n",
    "            g_loss_entropy = np.ones(shape) / np_cpu.prod(shape)#1\n",
    "            g_entropy_output = sigmoid_cross_entropy_with_logits_derv(y_one_list[i], output_list[i])#2\n",
    "\n",
    "            G_entropy = g_loss_entropy * G_loss\n",
    "            G_affine = g_entropy_output * G_entropy\n",
    "\n",
    "            G_affine = self.activate_derv(G_affine, output_list[i], {'actfunc':'sigmoid'})#3\n",
    "            g_affine_weight = x.transpose()\n",
    "            #print(pm['dic_end'][:,np.array(y_one_list[i],dtype='int32')].shape, G_affine.shape)\n",
    "            if i != 0:\n",
    "                g_affine_input = pm['dic_end'][:,np.array(temp_number_list, dtype='int32')[:,i-1].tolist()].transpose()\n",
    "                \n",
    "            else:\n",
    "                g_affine_input = pm['dic_end'][:,np.array(y,dtype='int32').tolist()].transpose()\n",
    "                \n",
    "            #print(\"wd\",G_affine.shape,g_affine_input.shape)\n",
    "            #recover_ = G_affine * np.ones([2500,300]) / 300  #####\n",
    "            #print(recover_)\n",
    "            G_weight = np.matmul(g_affine_weight, G_affine)\n",
    "            #print(self.temp_pm_false.shape)\n",
    "            \n",
    "            if i == self.negative:\n",
    "                G_input = np.matmul(G_affine, g_affine_input)\n",
    "            else:\n",
    "                G_input = G_input + np.matmul(G_affine, g_affine_input)  #5번으로 대체해도 될듯? 새로운 발명\n",
    "            \n",
    "            #G_input = np.matmul(G_affine, g_affine_input)    #5번\n",
    "            #print(\"종우\",G_input.shape)\n",
    "            self.inter_stop=1\n",
    "            self.update_param(pm, 'dic_end', G_weight)\n",
    "            self.inter_stop=0\n",
    "        return G_input.reshape(x_org_shape)\n",
    "    \n",
    "    def save_parameters(self,char):    #for word vector\n",
    "        time1 = int(time.time())\n",
    "\n",
    "        with open(\"parameters_\"+self.name+char+\".csv\",'w',newline='') as aa:\n",
    "            writer = csv.writer(aa, delimiter=',')\n",
    "            \n",
    "            if char == 'dic':\n",
    "                writer.writerow(self.hconfigs)\n",
    "                temp_pm=self.pm_hiddens\n",
    "                hconfigs=self.hconfigs\n",
    "            #print(self.pm_hiddens)\n",
    "        for i, j in enumerate(temp_pm):\n",
    "            #print(type_)\n",
    "            #print(i)\n",
    "            with open(\"parameters_\"+self.name+str(i)+'_'+char+\"_.csv\",'w',newline='') as aa:\n",
    "\n",
    "                writer = csv.writer(aa, delimiter=',')\n",
    "                \n",
    "                    \n",
    "                if i == 0:\n",
    "                    meta = self.pm_hiddens[i]['dic'].shape\n",
    "                    writer.writerow(meta)\n",
    "                    for k in range(meta[0]):\n",
    "                        writer.writerow(self.pm_hiddens[i]['dic'][k][:])\n",
    "                elif i ==1:\n",
    "                    meta = self.pm_hiddens[i]['w'].shape\n",
    "                    writer.writerow(meta)\n",
    "                    for k in range(meta[0]):\n",
    "                        writer.writerow(self.pm_hiddens[i]['w'][k][:])\n",
    "                    \n",
    "                    #writer.writerow(self.pm_hiddens[i]['w'])\n",
    "                    writer.writerow(self.pm_hiddens[i]['b'][:])\n",
    "                elif i == 2:\n",
    "                    meta = self.pm_hiddens[i]['dic_end'].shape\n",
    "                    writer.writerow(meta)\n",
    "                    for k in range(meta[0]):\n",
    "                        writer.writerow(self.pm_hiddens[i]['dic_end'][k][:])\n",
    "     \n",
    "        time2 = int(time.time())\n",
    "        print(time2-time1, \"걸린시간(초)\")\n",
    "\n",
    "    def load_parameters(self,char): #for word vector\n",
    "\n",
    "        time1 = int(time.time())\n",
    "        \n",
    "        if char == 'dic':\n",
    "            temp_pm=self.pm_hiddens\n",
    "            hconfigs=self.hconfigs\n",
    "        for i, j in enumerate(temp_pm):\n",
    "            #print(type_)\n",
    "\n",
    "            with open(\"parameters_\"+self.name+str(i)+'_'+char+\"_.csv\",'r',newline='') as aa:\n",
    "                #print(\"종우\")\n",
    "                reader = csv.reader(aa, delimiter=',')\n",
    "                w_list=list()\n",
    "                for k, t in enumerate(reader):                    \n",
    "                    if i==0:\n",
    "\n",
    "                        if k == 0:\n",
    "                            multi_0=int(t[0])\n",
    "         \n",
    "                            multi_1=int(t[1])\n",
    "                            continue\n",
    "                        elif k < multi_0: # if 100, multi_0 = 100, 실제 위치 101까지 있음. 그러므로 10\n",
    "                            t=list(map(float, t))\n",
    "            \n",
    "                            w_list.append(t)\n",
    "                  \n",
    "                        elif k == multi_0:\n",
    "                            t=list(map(float, t))\n",
    "                            #rint(\"asdasd\")\n",
    "                            w_list.append(t)\n",
    "                            #print(t)\n",
    "                            self.pm_hiddens[0]['dic']=np.array(w_list, dtype='float32')\n",
    "                            #print(self.pm_hiddens[0]['dic'].shape)\n",
    "                \n",
    "                    if i ==1:\n",
    "                        if k == 0:\n",
    "                            multi_0=int(t[0])\n",
    "                 \n",
    "                            multi_1=int(t[1])\n",
    "                            continue\n",
    "                        elif k < multi_0: # if 100, multi_0 = 100, 실제 위치 101까지 있음. 그러므로 10\n",
    "                            t=list(map(float, t))\n",
    "              \n",
    "                            w_list.append(t)\n",
    "       \n",
    "                        elif k == multi_0:\n",
    "                            t=list(map(float, t))\n",
    "                            w_list.append(t)\n",
    "                            self.pm_hiddens[1]['w']=np.array(w_list, dtype='float32')\n",
    "                            #print(t)\n",
    "                            print(self.pm_hiddens[1]['w'].shape)\n",
    "                        elif k == multi_0+1:\n",
    "                            t=list(map(float, t))\n",
    "                            #print(t)\n",
    "                            self.pm_hiddens[1]['b']=np.array(t,dtype='float32')\n",
    "                            #print(self.pm_hiddens[1]['w'].shape)\n",
    "                            \n",
    "                    if i==2:\n",
    "\n",
    "                        if k == 0:\n",
    "                            multi_0=int(t[0])\n",
    "         \n",
    "                            multi_1=int(t[1])\n",
    "                            continue\n",
    "                        elif k < multi_0: # if 100, multi_0 = 100, 실제 위치 101까지 있음. 그러므로 10\n",
    "                            t=list(map(float, t))\n",
    "            \n",
    "                            w_list.append(t)\n",
    "                  \n",
    "                        elif k == multi_0:\n",
    "                            t=list(map(float, t))\n",
    "                            w_list.append(t)\n",
    "                            self.pm_hiddens[2]['dic_end']=np.array(w_list, dtype='float32')\n",
    "                            #print(self.pm_hiddens[2]['dic_end'].shape)\n",
    "                \n",
    "                    \n",
    "                        \n",
    "        time2 = int(time.time())\n",
    "        print(time2-time1, \"걸린시간(초)\")\n",
    "                    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
