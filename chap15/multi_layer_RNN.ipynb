{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap15/gan.ipynb\n",
    "%run ../chap15/NLP_dataset.ipynb\n",
    "%run ../chap15/NLP_Doc.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "#RnnBasicModel('am_4',  ad, ['multi_layer_RNN', {'recur_size':4,  'outseq':False}])\n",
    "class multi_layer_RNN(NLP_Doc):\n",
    "    def __init__(self, name, dataset, hconfigs, JJ=0, show_maps=False,\n",
    "                 l2_decay=0, l1_decay=0, dump_structure=True, word_vector_dimension=100, window_size=1, negative=5, load=0, sentence_vector_dimension=100):\n",
    "        \n",
    "\n",
    "        super(multi_layer_RNN, self).__init__(name, dataset, hconfigs, show_maps,\n",
    "                                          l2_decay, l1_decay)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def alloc_multi_layer_RNN_layer(self, input_shape, hconfig):\n",
    "        inseq = get_conf_param(hconfig, 'inseq', True)\n",
    "        outseq = get_conf_param(hconfig, 'outseq', True)\n",
    "        use_state = get_conf_param(hconfig, 'use_state', False)\n",
    "        \n",
    "        if 'floor' in hconfig:\n",
    "            self.floor=len(hconfig['floor']) # 딕셔너리로 층 마다 노드 개수 등 입력받기 [1, {recur_size}\n",
    "        \"\"\"\n",
    "        a_={1:['1',{'re':10}]}\n",
    "        if 1 in a_:\n",
    "            print(a_[1][1]['re'])\n",
    "        \"\"\"\n",
    "        if inseq:\n",
    "            timesteps1, timefeats = input_shape\n",
    "        else:\n",
    "            timesteps1 = get_conf_param(hconfig, 'timesteps') + 1\n",
    "            timefeats = np_cpu.prod(input_shape)\n",
    "        for i in range(self.floor):\n",
    "            recur_size = get_conf_param(hconfig, 'recur_size')\n",
    "\n",
    "            ex_inp_dim = timefeats + recur_size\n",
    "            weight, bias = self.alloc_param_pair([ex_inp_dim, 4*recur_size])\n",
    "            bias[0*recur_size:1*recur_size] = 1.0\n",
    "\n",
    "            rnn_info = [inseq, outseq, timesteps1, timefeats, recur_size, use_state]\n",
    "            list_.append({'w':weight, 'b':bias, 'info':rnn_info})\n",
    "            \n",
    "        if outseq:\n",
    "                output_shape = [timesteps1, recur_size]\n",
    "            else:\n",
    "                output_shape = [recur_size]\n",
    "\n",
    "        return list_, output_shape\n",
    "\n",
    "\n",
    "    \n",
    "    def forward_multi_layer_RNN_layer(self, x, hconfig, pm):\n",
    "        inseq, outseq, timesteps1, timefeats, recur_size, use_state = pm['info']\n",
    "        mb_size = x.shape[0]\n",
    "\n",
    "        if inseq:\n",
    "            x_slices = x[:, 1:, :].transpose([1,0,2])\n",
    "            lengths = x[:, 0, 0].astype(np.int32)\n",
    "            max_length = int(np.max(lengths))\n",
    "        else:\n",
    "            x_slice = x\n",
    "            max_length = timesteps1 - 1\n",
    "            lengths = [max_length] * mb_size\n",
    "            \n",
    "        for i in range(self.floor):\n",
    "            pm[i]{'recurrent':np.zeros([mb_size, recur_size], dtype = 'float32')\n",
    "            pm[i]{'state' :np.zeros([mb_size, recur_size], dtype = 'float32')}\n",
    "            pm[i]{'outputs':[]}\n",
    "            pm[i]{'aux_step':[]}\n",
    "        aux_steps=[]\n",
    "\n",
    "        for n in range(max_length):\n",
    "            if inseq: x_slice = x_slices[n]\n",
    "\n",
    "            \n",
    "            for i in range(self.floor):\n",
    "                pm[i]{'ex_inp':np.hstack([x_slice, pm[i]['recurrent']])}\n",
    "                pm[i]{'affine':np.matmul(ex_inp, pm[i]['w']) + pm[i]['b']}\n",
    "\n",
    "                pm[i]{'forget_gate' : sigmoid(affine[:, 0*recur_size:1*recur_size])}\n",
    "                pm[i]{'input_gate'  : sigmoid(affine[:, 1*recur_size:2*recur_size])}\n",
    "                pm[i]{'output_gate' : sigmoid(affine[:, 2*recur_size:3*recur_size])}\n",
    "                pm[i]{'block_input' : tanh   (affine[:, 3*recur_size:4*recur_size])}\n",
    "\n",
    "                pm[i]{'state_tmp' : state}\n",
    "                pm[i]{'state': state_tmp * forget_gate + block_input * input_gate}\n",
    "\n",
    "                pm[i]{'recur_tmp ': tanh(state)}\n",
    "                pm[i]{'recurrent ': recur_tmp * output_gate}\n",
    "                x_slice=pm[i]['recurrent']\n",
    "                if use_state: pm[i]['outputs'].append(pm[i]['state'])\n",
    "                else: pm[i]['outputs'].append(pm[i]['recurrent']\n",
    "               \n",
    "                pm[i]{'aux_step':[pm[i]['ex_inp'], pm[i]['state_tmp'], pm[i]['block_input'], pm[i]['input_gate'], \\\n",
    "                            pm[i]['forget_gate'], pm[i]['output_gate'], pm[i]['recur_tmp']]}\n",
    "                aux_steps.append(pm[i]['aux_step'])\n",
    "                if i == len(self.floor)-1:\n",
    "                    outputs.append(pm[i]['outputs'].append(pm[i]['recurrent'])\n",
    "                                              \n",
    "                    \n",
    "\n",
    "        if outseq:\n",
    "            output = np.zeros([mb_size, timesteps1, recur_size], dtype = 'float32')\n",
    "            output[:, 0, 0] = lengths\n",
    "            output[:, 1:, :] = np.asarray(outputs).transpose([1, 0, 2])\n",
    "        else:\n",
    "            output = np.zeros([mb_size, recur_size], dtype = 'float32')\n",
    "            for n in range(mb_size):\n",
    "                output[n] = outputs[int(lengths[n]-1)][n]\n",
    "\n",
    "        return output, [x, lengths, max_length, outputs, aux_steps]\n",
    "\n",
    "\n",
    "    def backprop_multi_layer_RNN_layer(self, G_y, hconfig, pm, aux):\n",
    "        inseq, outseq, timesteps1, timefeats, recur_size, use_state = pm['info']\n",
    "        x, lengths, max_length, outputs, aux_steps = aux\n",
    "        mb_size = x.shape[0]\n",
    "\n",
    "        G_weight = np.zeros_like(pm['w'], dtype = 'float32')\n",
    "        G_bias = np.zeros_like(pm['b'], dtype = 'float32')\n",
    "        G_x = np.zeros(x.shape, dtype = 'float32')\n",
    "        G_recurrent = np.zeros([mb_size, recur_size], dtype = 'float32')\n",
    "        G_state = np.zeros([mb_size, recur_size], dtype = 'float32')\n",
    "\n",
    "        if inseq: G_x[:, 0, 0] = lengths\n",
    "\n",
    "        if outseq:\n",
    "            G_outputs = G_y[:, 1:, :].transpose([1, 0, 2])\n",
    "        else:\n",
    "            G_outputs = np.zeros([max_length, mb_size, recur_size], dtype = 'float32')\n",
    "            for n in range(mb_size):\n",
    "                G_outputs[lengths[n]-1, n, :] = G_y[n]\n",
    "\n",
    "        for n in reversed(range(0, max_length)):\n",
    "            #self.floor 곱연산 해야함.\n",
    "            if use_state: G_state += G_outputs[n]\n",
    "            else: G_recurrent += G_outputs[n]\n",
    "\n",
    "            ex_inp, state_tmp, block_input, input_gate, forget_gate, \\\n",
    "                                  output_gate, recur_tmp = aux_steps[n]\n",
    "\n",
    "            G_recur_tmp = G_recurrent * output_gate\n",
    "            G_output_gate = G_recurrent * recur_tmp\n",
    "\n",
    "            G_state += tanh_derv(recur_tmp) * G_recur_tmp\n",
    "\n",
    "            G_input_gate = G_state * block_input\n",
    "            G_block_input = G_state * input_gate\n",
    "\n",
    "            G_forget_gate = G_state * state_tmp\n",
    "            G_state = G_state * forget_gate\n",
    "\n",
    "            G_affine = np.zeros([mb_size, 4*recur_size], dtype = 'float32')\n",
    "\n",
    "\n",
    "            G_affine[:, 0*recur_size:1*recur_size] = \\\n",
    "                                    sigmoid_derv(forget_gate) * G_forget_gate\n",
    "            G_affine[:, 1*recur_size:2*recur_size] = \\\n",
    "                                    sigmoid_derv(input_gate)  * G_input_gate\n",
    "            G_affine[:, 2*recur_size:3*recur_size] = \\\n",
    "                                    sigmoid_derv(output_gate) * G_output_gate\n",
    "            G_affine[:, 3*recur_size:4*recur_size] = \\\n",
    "                                    tanh_derv   (block_input) * G_block_input\n",
    "\n",
    "            g_affine_weight = ex_inp.transpose()\n",
    "            g_affine_input = pm['w'].transpose()\n",
    "\n",
    "            G_weight += np.matmul(g_affine_weight, G_affine)\n",
    "            G_bias += np.sum(G_affine, axis=0)\n",
    "            G_ex_inp = np.matmul(G_affine, g_affine_input)\n",
    "\n",
    "            if inseq: G_x[:,n+1,:] = G_ex_inp[:, :timefeats]\n",
    "            else: G_x[:,:] += G_ex_inp[:, :timefeats]\n",
    "\n",
    "            G_recurrent = G_ex_inp[:, timefeats:]\n",
    "\n",
    "        self.update_param(pm, 'w', G_weight)\n",
    "        self.update_param(pm, 'b', G_bias)\n",
    "\n",
    "        return G_x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
