{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap15/NLP_Doc.ipynb\n",
    "%run ../chap15/transformer_dataset.ipynb\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "import scipy\n",
    "import time\n",
    "import re\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NLP_Doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-920e05dafe88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNLP_Doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     def __init__(self, name, dataset, hconfigs, JJ=0, show_maps=False, batch_size=batch_size, \n\u001b[0;32m      3\u001b[0m                  l2_decay=0, l1_decay=0, dump_structure=True, word_vector_dimension=512, window_size=1, negative=5, load=0,num_heads=8):\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vector_dimension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_vector_dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_unique_words_number\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NLP_Doc' is not defined"
     ]
    }
   ],
   "source": [
    "class transformer(NLP_Doc):\n",
    "    def __init__(self, name, dataset, hconfigs, JJ=0, show_maps=False, batch_size=1, \n",
    "                 l2_decay=0, l1_decay=0, dump_structure=True, word_vector_dimension=512, window_size=1, negative=5, load=0,num_heads=8):\n",
    "        self.word_vector_dimension=word_vector_dimension\n",
    "        self.training_unique_words_number=0\n",
    "        self.training_full_text_number=0\n",
    "        self.new_words_number=0\n",
    "        self.new_full_text_number=0\n",
    "        self.window_size=window_size\n",
    "        self.negative = negative\n",
    "        self.num_heads=num_heads\n",
    "        self.rand_std = 0.03\n",
    "        self.use_adam=1\n",
    "        self.inter_stop=0\n",
    "        self.l2_decay = 0\n",
    "        self.l1_decay = 0\n",
    "        self.dic_training=0\n",
    "        self.is_training=False\n",
    "        self.text_dataset  = transformer_Dataset(['initial'])\n",
    "        self.text_dataset.text_find_number_words(\"text_kor_eng.xlsx\",1)\n",
    "        self.load=load\n",
    "        self.batch_size=batch_size\n",
    "        self.look_ahead_index=0\n",
    "        self.name=name\n",
    "        self.average_normal=0\n",
    "        self.ffnn_size=300\n",
    "        self.warmup_steps=1000\n",
    "        self.init_parameters(hconfigs)\n",
    "        self.root_=np.sqrt(int(self.word_vector_dimension/self.num_heads))\n",
    "        self.n=0\n",
    "        self.position_encoder()\n",
    "        self.position_decoder()\n",
    "        #self.load_parameters('dic')\n",
    "        self.G_input_encoder=0\n",
    "        #self.save_parameters('dic')\n",
    "        self.predict=0\n",
    "        self.bleu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(self, hconfigs):\n",
    "    self.hconfigs = hconfigs\n",
    "    self.pm_hiddens = []\n",
    "    pm_hidden=self.alloc_embedding_layer()\n",
    "    self.pm_hiddens.append(pm_hidden)\n",
    "    \n",
    "    prev_shape=(self.text_dataset.most_length,self.word_vector_dimension)\n",
    "    for i in range(self.hconfigs[0][1]['repeat']):\n",
    "        pm_hidden, prev_shape = self.alloc_multi_heads_layer(0)\n",
    "        self.pm_hiddens.append(pm_hidden)\n",
    "    pm_hidden=self.alloc_target_embedding_layer()\n",
    "    self.pm_hiddens.append(pm_hidden)\n",
    "    for i in range(self.hconfigs[1][1]['repeat']):\n",
    "        pm_hidden, prev_shape = self.alloc_multi_heads_layer(1)\n",
    "        self.pm_hiddens.append(pm_hidden)\n",
    "    dic_temp={}\n",
    "    dic_temp['w'], dic_temp['b']=transformer.making_weight(self,self.word_vector_dimension,len(self.text_dataset.target_id_to_word))\n",
    "    self.pm_hiddens.append(dic_temp)\n",
    "        \n",
    "transformer.init_parameters = init_parameters\n",
    "\n",
    "def making_weight_em(self, row, column):\n",
    "    np.random.seed(int(time.time()*10000000))\n",
    "    weight= np.random.normal(self.average_normal, self.rand_std, [row, column], dtype = 'float32')\n",
    "    time.sleep(0.001)\n",
    "    return weight\n",
    "\n",
    "def making_weight(self, row, column): #1d\n",
    "    np.random.seed(int(time.time()*10000000))\n",
    "    weight= np.random.normal(self.average_normal, self.rand_std, [row, column], dtype = 'float32')\n",
    "    time.sleep(0.001)\n",
    "    bias = np.zeros([column], dtype = 'float32')\n",
    "    return weight, bias\n",
    "\n",
    "transformer.making_weight_em=making_weight_em\n",
    "transformer.making_weight=making_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alloc_multi_heads_layer(self,decoder=0):\n",
    "    temp_list=[]\n",
    "    \n",
    "    if self.word_vector_dimension%self.num_heads !=0:\n",
    "        raise NameError('self.word_vector_dimension%self.num_heads')\n",
    "    \n",
    "    input_cnt = np_cpu.prod(self.word_vector_dimension)\n",
    "    output_cnt = int(self.word_vector_dimension/self.num_heads)\n",
    "    print(\"self.num_heads:\",self.num_heads,output_cnt)\n",
    "    for i in range(self.num_heads):\n",
    "        temp_dic={}\n",
    "        temp_dic[str(i)+'_q'],temp_dic[str(i)+'_qb']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "        temp_dic[str(i)+'_k'],temp_dic[str(i)+'_kb']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "        temp_dic[str(i)+'_v'],temp_dic[str(i)+'_vb']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "\n",
    "        if decoder ==1:\n",
    "            temp_dic[str(i)+'_q2'],temp_dic[str(i)+'_q2b']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "            temp_dic[str(i)+'_k2'],temp_dic[str(i)+'_k2b']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "            temp_dic[str(i)+'_v2'],temp_dic[str(i)+'_v2b']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "        temp_list.append(temp_dic)\n",
    "    \n",
    "    temp_dic={}\n",
    "    temp_dic['0_w'],temp_dic['0_wb']=transformer.making_weight(self,input_cnt, input_cnt)\n",
    "    temp_dic['1_w'],temp_dic['1_wb']=transformer.making_weight(self,input_cnt,self.ffnn_size)\n",
    "    temp_dic['2_w'],temp_dic['2_wb']=transformer.making_weight(self,self.ffnn_size,input_cnt)\n",
    "    temp_dic['2_batch'],_=transformer.alloc_batch_normal_layer(self,input_cnt,None)\n",
    "    temp_dic['0_batch'],_=transformer.alloc_batch_normal_layer(self,input_cnt,None)\n",
    "    if decoder == 1:\n",
    "        temp_dic['3_w'],temp_dic['3_wb']=transformer.making_weight(self,input_cnt, input_cnt)\n",
    "        temp_dic['1_batch'],_=transformer.alloc_batch_normal_layer (self,input_cnt,None)\n",
    "    temp_list.append(temp_dic)\n",
    "    \n",
    "    return temp_list, (self.batch_size,self.text_dataset.most_length,self.word_vector_dimension)\n",
    "\n",
    "transformer.alloc_multi_heads_layer=alloc_multi_heads_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self,epoch_count, learning_rate, learning_decrease,restore, name):\n",
    "    print(\"the number of words and the number of unique words\")\n",
    "    print(len(self.text_dataset.original_text),self.text_dataset.original_unique_words)\n",
    "    print(\"target-the number of words and the number of unique words\")\n",
    "    print(len(self.text_dataset.target_text),self.text_dataset.target_unique_words)\n",
    "    print(self.text_dataset.original_id_to_word)\n",
    "    print(self.text_dataset.target_id_to_word)\n",
    "    print(self.text_dataset.most_length_target)\n",
    "    print(self.text_dataset.most_length)\n",
    "    if self.load == 1:\n",
    "        self.load_parameters(\"dic\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    batch_count = int(self.text_dataset.train_count / self.batch_size)\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        bleu = []\n",
    "        bleu_ = []\n",
    "        self.epoch = epoch+1\n",
    "        print(\"self.epoch\",self.epoch)\n",
    "        #self.learning_rate = self.word_vector_dimension**(-0.5)*np.min(np.array([self.epoch**(-0.5),self.epoch*self.warmup_steps**(-1.5)]))\n",
    "        #print(self.learning_rate)\n",
    "        self.learning_rate=learning_rate*learning_decrease\n",
    "        self.text_dataset.shuffle_train_data(self.batch_size*batch_count)\n",
    "        \n",
    "        for n in range(batch_count):\n",
    "            self.n=n\n",
    "            \n",
    "            trX, trY = self.text_dataset.get_train_data(self.batch_size, n)\n",
    "            cost,bleu_item= self.train_step(trX, trY)\n",
    "            bleu.append(bleu_item)\n",
    "            print(\"loss: \",np.average(cost))\n",
    "            if self.epoch%20==0 and self.epoch!=0:\n",
    "                bleu_.append(self.bleu_)\n",
    "            #print(\"악\",trX)\n",
    "        print(\"bleu: \",np.average(np.array(bleu)))\n",
    "        if self.epoch%20==0 and  self.epoch!=0:\n",
    "            print(\"bleu_real: \",np.average(np.array(bleu_)))\n",
    "        if epoch % 5 ==0and epoch !=0:\n",
    "            save_parameters(self,'dic')\n",
    "        \n",
    "    tm_total = int(time.time()) - time1\n",
    "    print('Model {} train ended in {} secs:'.format(self.name, tm_total))\n",
    "\n",
    "transformer.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(self,char):    #for word vector\n",
    "\n",
    "    time1 = int(time.time())\n",
    "\n",
    "    with open(\"hyper_\"+self.name+\".csv\",'w',newline='') as aa:\n",
    "        \n",
    "        print(self.hconfigs, file=aa)\n",
    "        print(self.word_vector_dimension, file=aa)\n",
    "        print(self.training_unique_words_number,file=aa)\n",
    "        print(self.training_full_text_number,file=aa)\n",
    "        print(self.new_words_number,file=aa)\n",
    "        print(self.new_full_text_number,file=aa)\n",
    "        print(self.batch_size,file=aa)\n",
    "        print(self.ffnn_size,file=aa)\n",
    "        print(self.pm_hiddens[0]['dic'].shape,file=aa)\n",
    "        \n",
    "        #print(self.pm_hiddens)\n",
    "    with open(\"dic_original_\"+self.name+\"_.csv\",'w',newline='') as aa:\n",
    "        for i in range(len(self.text_dataset.original_id_to_word)):\n",
    "            print(self.text_dataset.original_id_to_word[i], file=aa)\n",
    "            \n",
    "    with open(\"dic_target_\"+self.name+\"_.csv\",'w',newline='') as aa:\n",
    "        for i in range(len(self.text_dataset.target_id_to_word)):\n",
    "            print(self.text_dataset.target_id_to_word[i], file=aa)\n",
    "            \n",
    "    self.save_ass(\"word_vector_original_\",self.pm_hiddens[0]['dic'])\n",
    "    \n",
    "    for j in range(self.hconfigs[0][1]['repeat']):\n",
    "        pm=self.pm_hiddens[j+1]\n",
    "        for i in range(self.num_heads):\n",
    "            self.save_ass(\"encoder\"+str(j)+str(i)+\"below_q\",pm[i][str(i)+'_q'])\n",
    "            self.save_ass(\"encoder\"+str(j)+str(i)+\"below_k\",pm[i][str(i)+'_k'])\n",
    "            self.save_ass(\"encoder\"+str(j)+str(i)+\"below_v\",pm[i][str(i)+'_v'])\n",
    "            self.save_ass_1(\"encoder\"+str(j)+str(i)+\"below_qb\",pm[i][str(i)+'_qb'])\n",
    "            self.save_ass_1(\"encoder\"+str(j)+str(i)+\"below_kb\",pm[i][str(i)+'_kb'])\n",
    "            self.save_ass_1(\"encoder\"+str(j)+str(i)+\"below_vb\",pm[i][str(i)+'_vb'])\n",
    "\n",
    "        self.save_ass(\"encoder\"+str(j)+\"upper_0_w\",pm[-1]['0_w'])\n",
    "        self.save_ass(\"encoder\"+str(j)+\"upper_1_w\",pm[-1]['1_w'])\n",
    "        self.save_ass(\"encoder\"+str(j)+\"upper_2_w\",pm[-1]['2_w'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_0_wb\",pm[-1]['0_wb'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_1_wb\",pm[-1]['1_wb'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_2_wb\",pm[-1]['2_wb'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_0_batch_mavg\",pm[-1]['0_batch']['mavg'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_0_batch_mvar\",pm[-1]['0_batch']['mvar'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_2_batch_mavg\",pm[-1]['2_batch']['mavg'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_2_batch_mvar\",pm[-1]['2_batch']['mvar'])\n",
    "\n",
    "    self.save_ass(\"word_vector_target_\",self.pm_hiddens[1+self.hconfigs[0][1]['repeat']]['dic'])\n",
    "    \n",
    "    for j in range(self.hconfigs[0][1]['repeat']):\n",
    "        pm=self.pm_hiddens[2+self.hconfigs[0][1]['repeat']+j]\n",
    "        for i in range(self.num_heads):\n",
    "            self.save_ass(\"decoder\"+str(j)+str(i)+\"below_q\",pm[i][str(i)+'_q'])\n",
    "            self.save_ass(\"decoder\"+str(j)+str(i)+\"below_k\",pm[i][str(i)+'_k'])\n",
    "            self.save_ass(\"decoder\"+str(j)+str(i)+\"below_v\",pm[i][str(i)+'_v'])\n",
    "            self.save_ass_1(\"decoder\"+str(j)+str(i)+\"below_qb\",pm[i][str(i)+'_qb'])\n",
    "            self.save_ass_1(\"decoder\"+str(j)+str(i)+\"below_kb\",pm[i][str(i)+'_kb'])\n",
    "            self.save_ass_1(\"decoder\"+str(j)+str(i)+\"below_vb\",pm[i][str(i)+'_vb'])\n",
    "           \n",
    "            self.save_ass(\"decoder\"+str(j)+str(i)+\"middel_q\",pm[i][str(i)+'_q2'])\n",
    "            self.save_ass(\"decoder\"+str(j)+str(i)+\"middel_k\",pm[i][str(i)+'_k2'])\n",
    "            self.save_ass(\"decoder\"+str(j)+str(i)+\"middel_v\",pm[i][str(i)+'_v2'])\n",
    "            self.save_ass_1(\"decoder\"+str(j)+str(i)+\"middel_qb\",pm[i][str(i)+'_q2b'])\n",
    "            self.save_ass_1(\"decoder\"+str(j)+str(i)+\"middel_kb\",pm[i][str(i)+'_k2b'])\n",
    "            self.save_ass_1(\"decoder\"+str(j)+str(i)+\"middel_vb\",pm[i][str(i)+'_v2b'])\n",
    "   \n",
    "        self.save_ass(\"decoder\"+str(j)+\"upper_0_w\",pm[-1]['0_w'])\n",
    "        self.save_ass(\"decoder\"+str(j)+\"upper_1_w\",pm[-1]['1_w'])\n",
    "        self.save_ass(\"decoder\"+str(j)+\"upper_2_w\",pm[-1]['2_w'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_0_wb\",pm[-1]['0_wb'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_1_wb\",pm[-1]['1_wb'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_2_wb\",pm[-1]['2_wb'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_0_batch_mavg\",pm[-1]['0_batch']['mavg'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_0_batch_mvar\",pm[-1]['0_batch']['mvar'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_1_batch_mavg\",pm[-1]['1_batch']['mavg'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_1_batch_mvar\",pm[-1]['1_batch']['mvar'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_2_batch_mavg\",pm[-1]['2_batch']['mavg'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_2_batch_mvar\",pm[-1]['2_batch']['mvar'])\n",
    "        self.save_ass(\"decoder\"+str(j)+\"upper_3_w\",pm[-1]['3_w'])\n",
    "        self.save_ass_1(\"decoder\"+str(j)+\"upper_3_wb\",pm[-1]['3_wb'])\n",
    " \n",
    "            \n",
    "  \n",
    "    self.save_ass(\"output_\",self.pm_hiddens[-1]['w'])\n",
    "    self.save_ass_1(\"output_b\",self.pm_hiddens[-1]['b'])\n",
    "   \n",
    "    \n",
    "            \n",
    "            \n",
    "def save_ass(self,name,pm):\n",
    "    with open(name+\"_.csv\",'w',newline='') as aa:\n",
    "        temp_list=list()\n",
    "        for i in range(pm.shape[0]):\n",
    "            temp_list=list()\n",
    "            re_before=pm[i,:].tolist()\n",
    "            for j in range(len(re_before)):\n",
    "                re_before[j]=re.sub('\\\\n','',str(re_before[j]))\n",
    "                temp_list.append(re_before[j])\n",
    "            print(temp_list, file=aa)\n",
    "\n",
    "def save_ass_1(self,name,pm):\n",
    "    with open(name+\"_.csv\",'w',newline='') as aa:\n",
    "        re_before=pm.tolist()\n",
    "        for j in range(len(re_before)):\n",
    "            re_before[j]=re.sub('\\\\n','',str(re_before[j]))\n",
    " \n",
    "        print(re_before, file=aa)\n",
    "            \n",
    "def load_parameters(self,char): #for word vector\n",
    "    self.load_ass(\"word_vector_original_\",self.pm_hiddens[0]['dic'])\n",
    "    \n",
    "    for j in range(self.hconfigs[0][1]['repeat']):\n",
    "        pm=self.pm_hiddens[j+1]\n",
    "        for i in range(self.num_heads):\n",
    "            self.load_ass(\"encoder\"+str(j)+str(i)+\"below_q\",pm[i][str(i)+'_q'])\n",
    "            self.load_ass(\"encoder\"+str(j)+str(i)+\"below_k\",pm[i][str(i)+'_k'])\n",
    "            self.load_ass(\"encoder\"+str(j)+str(i)+\"below_v\",pm[i][str(i)+'_v'])\n",
    "            self.load_ass_1(\"encoder\"+str(j)+str(i)+\"below_qb\",pm[i][str(i)+'_qb'])\n",
    "            self.load_ass_1(\"encoder\"+str(j)+str(i)+\"below_kb\",pm[i][str(i)+'_kb'])\n",
    "            self.load_ass_1(\"encoder\"+str(j)+str(i)+\"below_vb\",pm[i][str(i)+'_vb'])\n",
    "\n",
    "        self.load_ass(\"encoder\"+str(j)+\"upper_0_w\",pm[-1]['0_w'])\n",
    "        self.load_ass(\"encoder\"+str(j)+\"upper_1_w\",pm[-1]['1_w'])\n",
    "        self.load_ass(\"encoder\"+str(j)+\"upper_2_w\",pm[-1]['2_w'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_0_wb\",pm[-1]['0_wb'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_1_wb\",pm[-1]['1_wb'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_2_wb\",pm[-1]['2_wb'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_0_batch_mavg\",pm[-1]['0_batch']['mavg'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_0_batch_mvar\",pm[-1]['0_batch']['mvar'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_2_batch_mavg\",pm[-1]['2_batch']['mavg'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_2_batch_mvar\",pm[-1]['2_batch']['mvar'])\n",
    "    \n",
    "    self.load_ass(\"word_vector_target_\",self.pm_hiddens[1+self.hconfigs[0][1]['repeat']]['dic'])\n",
    "                       \n",
    "    for j in range(self.hconfigs[0][1]['repeat']):\n",
    "        pm=self.pm_hiddens[2+self.hconfigs[0][1]['repeat']+j]\n",
    "        for i in range(self.num_heads):\n",
    "            self.load_ass(\"decoder\"+str(j)+str(i)+\"below_q\",pm[i][str(i)+'_q'])\n",
    "            self.load_ass(\"decoder\"+str(j)+str(i)+\"below_k\",pm[i][str(i)+'_k'])\n",
    "            self.load_ass(\"decoder\"+str(j)+str(i)+\"below_v\",pm[i][str(i)+'_v'])\n",
    "           \n",
    "            self.load_ass(\"decoder\"+str(j)+str(i)+\"middel_q\",pm[i][str(i)+'_q2'])\n",
    "            self.load_ass(\"decoder\"+str(j)+str(i)+\"middel_k\",pm[i][str(i)+'_k2'])\n",
    "            self.load_ass(\"decoder\"+str(j)+str(i)+\"middel_v\",pm[i][str(i)+'_v2'])\n",
    "            \n",
    "            self.load_ass_1(\"decoder\"+str(j)+str(i)+\"below_qb\",pm[i][str(i)+'_qb'])\n",
    "            self.load_ass_1(\"decoder\"+str(j)+str(i)+\"below_kb\",pm[i][str(i)+'_kb'])\n",
    "            self.load_ass_1(\"decoder\"+str(j)+str(i)+\"below_vb\",pm[i][str(i)+'_vb'])\n",
    "           \n",
    "            self.load_ass_1(\"decoder\"+str(j)+str(i)+\"middel_qb\",pm[i][str(i)+'_q2b'])\n",
    "            self.load_ass_1(\"decoder\"+str(j)+str(i)+\"middel_kb\",pm[i][str(i)+'_k2b'])\n",
    "            self.load_ass_1(\"decoder\"+str(j)+str(i)+\"middel_vb\",pm[i][str(i)+'_v2b'])\n",
    "   \n",
    "        self.load_ass(\"decoder\"+str(j)+\"upper_0_w\",pm[-1]['0_w'])\n",
    "        self.load_ass(\"decoder\"+str(j)+\"upper_1_w\",pm[-1]['1_w'])\n",
    "        self.load_ass(\"decoder\"+str(j)+\"upper_2_w\",pm[-1]['2_w'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_0_wb\",pm[-1]['0_wb'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_1_wb\",pm[-1]['1_wb'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_2_wb\",pm[-1]['2_wb'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_0_batch_mavg\",pm[-1]['0_batch']['mavg'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_0_batch_mvar\",pm[-1]['0_batch']['mvar'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_1_batch_mavg\",pm[-1]['1_batch']['mavg'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_1_batch_mvar\",pm[-1]['1_batch']['mvar'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_2_batch_mavg\",pm[-1]['2_batch']['mavg'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_2_batch_mvar\",pm[-1]['2_batch']['mvar'])\n",
    "        self.load_ass(\"decoder\"+str(j)+\"upper_3_w\",pm[-1]['3_w'])\n",
    "        self.load_ass_1(\"decoder\"+str(j)+\"upper_3_wb\",pm[-1]['3_wb'])\n",
    " \n",
    "            \n",
    "    self.load_ass(\"output_\",self.pm_hiddens[-1]['w'])\n",
    "    self.load_ass_1(\"output_b\",self.pm_hiddens[-1]['b'])\n",
    "\n",
    "\n",
    "def load_ass(self,name,pm):\n",
    "    print(name)\n",
    "    with open(name+\"_.csv\",'r',newline='') as aa:      \n",
    "        for i in range(pm.shape[0]):\n",
    "            temp_line=aa.readline().split(',')\n",
    "        \n",
    "            for j in range(len(temp_line)):\n",
    "                if j ==len(temp_line)-1:\n",
    "                    try:\n",
    "                        pm[i,j]=float(temp_line[j][2:-3])\n",
    "                    except:\n",
    "                        print(temp_line)\n",
    "                else:\n",
    "                    try:\n",
    "                        pm[i,j]=float(temp_line[j][2:-1])\n",
    "                    except:\n",
    "                        print(temp_line)\n",
    "\n",
    "def load_ass_1(self,name,pm):\n",
    "    print(name)\n",
    "    with open(name+\"_.csv\",'r',newline='') as aa:      \n",
    "        temp_line=aa.readline().split(',')\n",
    "\n",
    "        for j in range(len(temp_line)):\n",
    "            if j ==len(temp_line)-1:\n",
    "                pm[j]=float(temp_line[j][2:-3])\n",
    "            else:\n",
    "                pm[j]=float(temp_line[j][2:-1])\n",
    "    \n",
    "transformer.load_ass_1=load_ass_1\n",
    "transformer.load_ass=load_ass\n",
    "transformer.save_ass_1=save_ass_1\n",
    "transformer.save_ass=save_ass\n",
    "transformer.load_parameters=load_parameters\n",
    "transformer.save_parameters=save_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def train_step(self, x, y):\n",
    "    loss_temp=list()\n",
    "    \n",
    "    self.first_time=1\n",
    "    self.is_training = True\n",
    "    loss, aux_nn = self.forward_neuralnet(x,y)\n",
    "    loss_temp.append(loss)\n",
    "    G_loss = 1.0\n",
    "    self.backprop_neuralnet(G_loss, aux_nn)\n",
    "    self.is_training = False\n",
    "    if self.epoch%20==0 and self.epoch!=0:\n",
    "        self.predict_translation(x,y)\n",
    "    \n",
    "    self.bleu=0\n",
    "    y=np.concatenate((y[:,1:],self.text_dataset.target_unique_words-2+np.zeros((y.shape[0],1),dtype=\"int32\")),axis=1)\n",
    "    #print(y)\n",
    "    a=y.tolist()\n",
    "    b=[]\n",
    "    for i in range(self.batch_size):\n",
    "        b.append(np.argmax(softmax(self.output[i]),axis=1).tolist())\n",
    "    #print(np.argmax(softmax(self.output[0]),axis=1).tolist())\n",
    "    temp_list_bleu=list()\n",
    "    temp_list_bleu_b=list()\n",
    "    for i in range(self.batch_size):\n",
    "        temp=[]\n",
    "        for j in range(len(a[0])):\n",
    "            temp_=self.text_dataset.target_id_to_word[a[i][j]]\n",
    "            if temp_!='sep0':\n",
    "                temp.append(temp_)\n",
    "        temp_list_bleu.append(temp)\n",
    "    \n",
    "    for i in range(self.batch_size):\n",
    "        temp=[]\n",
    "        for j in range(len(b[0])):\n",
    "            temp_=self.text_dataset.target_id_to_word[b[i][j]]\n",
    "            if temp_!='sep0':\n",
    "                temp.append(temp_)\n",
    "        temp_list_bleu_b.append(temp)\n",
    "    #print([temp_list_bleu[0]],temp_list_bleu_b[0])\n",
    "    #print(sentence_bleu([temp_list_bleu[0]],temp_list_bleu_b[0]))\n",
    "    #print(\"헉\",sentence_bleu([['1']],['1']))\n",
    "    for i in range(self.batch_size):\n",
    "        self.bleu+=sentence_bleu([temp_list_bleu[i]],temp_list_bleu_b[i])\n",
    "    self.bleu/=self.batch_size\n",
    "    #print(self.bleu)\n",
    "    #print(temp_list_bleu)\n",
    "    #result_list_list.append(self.text_dataset.target_id_to_word[w])\n",
    "    return np.mean(np.array(loss_temp)), self.bleu\n",
    "\n",
    "transformer.train_step=train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_neuralnet(self, x,y):\n",
    "    hidden = x\n",
    "    #print(x[0])\n",
    "    aux_layers = [] #embedding, encoder, decoder 1ch/  embedding [interlalyer] output\n",
    "    hidden, aux = self.forward_embedding_layer(hidden, None, self.pm_hiddens[0])\n",
    "    #print(self.pm_hiddens[0])\n",
    "    \n",
    "    aux_layers.append(aux)\n",
    "    aux_layers_temp=[]\n",
    "    \n",
    "    for i in range(self.hconfigs[0][1]['repeat']):\n",
    "        hidden, aux = self.forward_multi_heads_layer(hidden, self.pm_hiddens[i+1],0)\n",
    "        aux_layers_temp.append(aux)\n",
    "    #print(\"nomal\",hidden)\n",
    "    #print(hidden[0])\n",
    "    aux_layers.append(aux_layers_temp)\n",
    "    self.incoder_output=hidden\n",
    "    #print(y)\n",
    "    hidden, aux = self.forward_target_embedding_layer(y, None, self.pm_hiddens[1+self.hconfigs[0][1]['repeat']])\n",
    "    aux_layers.append(aux)\n",
    "    aux_layers_temp=[]\n",
    "    \n",
    "    for j in range(0,self.hconfigs[1][1]['repeat'],1):\n",
    "        hidden, aux = self.forward_multi_heads_layer(hidden,self.pm_hiddens[2+self.hconfigs[0][1]['repeat']+j],1)\n",
    "        aux_layers_temp.append(aux)\n",
    "    \n",
    "    aux_layers.append(aux_layers_temp)\n",
    "    aux_layers.append(hidden)\n",
    "    output=np.matmul(hidden,self.pm_hiddens[-1]['w'])+self.pm_hiddens[-1]['b']\n",
    "    self.output=np.copy(output)\n",
    "    result__=np.zeros(y.shape)\n",
    "    \n",
    "        \n",
    "    #print(\"output\",np.argmax(softmax(output[0]), axis=1))\n",
    "    #print(\"input_target\",y[0])\n",
    "    y=np.concatenate((y[:,1:],self.text_dataset.target_unique_words-2+np.zeros((y.shape[0],1),dtype=\"int32\")),axis=1)\n",
    "    #print(\"target\",y[0])\n",
    "    #for i in range(y.shape[0]):\n",
    "        #if y[i]==26:\n",
    "            #print(\"e\")\n",
    "    #print(y.shape,output.shape)\n",
    "    for i in range(y.shape[0]):\n",
    "        result__[i]=softmax_cross_entropy_with_logits(self.text_dataset.training_target_one_hot_word[y[i,:]]\n",
    "                                                              , output[i,:,:])\n",
    "        #if i==0 and self.epoch %10==0:\n",
    "        #    print(self.text_dataset.training_target_one_hot_word[y[i,:]], softmax(output[i,:,:]))\n",
    "\n",
    "    return np.mean(result__), [aux_layers, output, result__,self.text_dataset.training_target_one_hot_word[y]]\n",
    "\n",
    "transformer.forward_neuralnet=forward_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forward_neuralnet(self, x,t):\n",
    "    \n",
    "    \n",
    "    hidden = x\n",
    "    #print(\"predict-x\", hidden)\n",
    "    aux_layers = [] #embedding, encoder, decoder 1ch/  embedding [interlalyer] output\n",
    "    hidden, _ = self.forward_embedding_layer(hidden, None, self.pm_hiddens[0])\n",
    "    #\n",
    "    \n",
    "    for i in range(self.hconfigs[0][1]['repeat']):\n",
    "        hidden, _ = self.forward_multi_heads_layer(hidden, self.pm_hiddens[i+1],0)\n",
    "    #print(\"Few\",hidden)\n",
    "    self.incoder_output=hidden\n",
    "\n",
    "    output_stack=[]\n",
    "    aux_layers_temp=[]\n",
    "    y=np.zeros((self.batch_size,self.text_dataset.most_length_target),dtype='int64')+self.text_dataset.target_unique_words-1\n",
    "    \n",
    "    self.predict=1\n",
    "    for i in range(self.text_dataset.most_length_target-1):\n",
    "        self.epoch_i=i\n",
    "\n",
    "        hidden, _ = self.forward_target_embedding_layer(y, None, self.pm_hiddens[1+self.hconfigs[0][1]['repeat']])\n",
    "        \n",
    "        for j in range(0,self.hconfigs[1][1]['repeat'],1):\n",
    "            hidden, _ = self.forward_multi_heads_layer(hidden,self.pm_hiddens[2+self.hconfigs[0][1]['repeat']+j],1)\n",
    "            \n",
    "\n",
    "        output=np.matmul(hidden,self.pm_hiddens[-1]['w'])+self.pm_hiddens[-1]['b']\n",
    "        output_softmax=np.zeros(output.shape)\n",
    "\n",
    "        for j in range(output.shape[0]):\n",
    "            output_softmax[j,:,:]=softmax(output[j,:,:])\n",
    "\n",
    "        output_softmax=np.argmax(output_softmax,axis=2)\n",
    "\n",
    "        y[:,i+1]=output_softmax[:,self.epoch_i]\n",
    "\n",
    "    self.predict=0\n",
    "    #print(\"predict-y\",y)\n",
    "    a=t.tolist()\n",
    "    b=y.tolist()\n",
    "    \n",
    "    #print(np.argmax(softmax(self.output[0]),axis=1).tolist())\n",
    "    temp_list_bleu=list()\n",
    "    temp_list_bleu_b=list()\n",
    "    for i in range(self.batch_size):\n",
    "        temp=[]\n",
    "        for j in range(len(a[0])):\n",
    "            temp_=self.text_dataset.target_id_to_word[a[i][j]]\n",
    "            if temp_!='sep0':\n",
    "                temp.append(temp_)\n",
    "        temp_list_bleu.append(temp)\n",
    "    \n",
    "    for i in range(self.batch_size):\n",
    "        temp=[]\n",
    "        for j in range(len(b[0])):\n",
    "            temp_=self.text_dataset.target_id_to_word[b[i][j]]\n",
    "            if temp_!='sep0':\n",
    "                temp.append(temp_)\n",
    "        temp_list_bleu_b.append(temp)\n",
    "    #print([temp_list_bleu[0]],temp_list_bleu_b[0])\n",
    "    #print(sentence_bleu([temp_list_bleu[0]],temp_list_bleu_b[0]))\n",
    "    #print(\"헉\",sentence_bleu([['1']],['1']))\n",
    "    self.bleu_=0\n",
    "    for i in range(self.batch_size):\n",
    "        self.bleu_+=sentence_bleu([temp_list_bleu[i]],temp_list_bleu_b[i])\n",
    "    self.bleu_/=self.batch_size\n",
    "    #print(\"prediction_finish\")\n",
    "    \n",
    "    return y\n",
    "    \n",
    "    \n",
    "transformer.predict_forward_neuralnet=predict_forward_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d1a83bab831b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mG_hidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop_neuralnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackprop_neuralnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "def backprop_neuralnet(self, G_loss, aux):\n",
    "    \n",
    "    aux_layers, output, result__, y= aux\n",
    "\n",
    "    g_loss_entropy = 1.0 / np_cpu.prod(result__.shape)\n",
    "    g_entropy_output=np.zeros(y.shape)\n",
    "    for i in range(y.shape[0]):\n",
    "        \n",
    "        g_entropy_output[i] = softmax_cross_entropy_with_logits_derv(y[i,:,:], output[i,:,:])\n",
    "    G_entropy = g_loss_entropy * G_loss\n",
    "    G_affine = g_entropy_output * G_entropy\n",
    "    G_bias=np.sum(G_affine, axis = (0,1))\n",
    "    g_affine_input = self.pm_hiddens[-1]['w'].transpose()\n",
    "    \n",
    "    for i in range(G_affine.shape[0]):\n",
    "        if i ==0:\n",
    "            G_weight_=np.matmul(aux_layers[-1][i].transpose(),G_affine[i])\n",
    "        else:\n",
    "            G_weight_+=np.matmul(aux_layers[-1][i].transpose(),G_affine[i])\n",
    "\n",
    "    self.update_param(self.pm_hiddens[-1], 'w', G_weight_)\n",
    "    self.update_param(self.pm_hiddens[-1], 'b', G_bias)\n",
    "    G_hidden = np.matmul(G_affine, g_affine_input)\n",
    "    aux_layers_temp_1=aux_layers[-2]\n",
    "    #print(G_hidden)\n",
    "    #if self.epoch>1000:\n",
    "    \n",
    "    for j in reversed(range(0,self.hconfigs[1][1]['repeat'],1)):\n",
    "        pm, aux =  self.pm_hiddens[2+self.hconfigs[0][1]['repeat']+j], aux_layers_temp_1[j]\n",
    "        G_hidden = self.backprop_multi_heads_layer(G_hidden,  pm, aux,1)\n",
    "    \n",
    "    \n",
    "    G_hidden = self.backprop_target_embedding_layer(G_hidden, None, self.pm_hiddens[1+self.hconfigs[0][1]['repeat']], aux_layers[-3])\n",
    "\n",
    "    aux_layers_temp=aux_layers[1]\n",
    "    G_hidden = self.G_input_encoder\n",
    "\n",
    "    \n",
    "    for i in reversed(range(self.hconfigs[0][1]['repeat'])):\n",
    "        pm, aux = self.pm_hiddens[i+1], aux_layers_temp[i]\n",
    "        G_hidden = self.backprop_multi_heads_layer(G_hidden, pm,aux,0)\n",
    "\n",
    "    \n",
    "    aux_layers_temp=aux_layers[0]\n",
    "    G_hidden = self.backprop_embedding_layer(G_hidden, None, self.pm_hiddens[0],aux_layers_temp)\n",
    "    \n",
    "    return G_hidden\n",
    "\n",
    "transformer.backprop_neuralnet=backprop_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-357a2ad0c91e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_attention_conca\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maux_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_multi_heads_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforward_multi_heads_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "def forward_multi_heads_layer(self, x, pm,decoder=0):\n",
    "    \n",
    "    aux_layers=[]\n",
    "    for i in range(self.num_heads):\n",
    "        q_matrix=np.matmul(x,pm[i][str(i)+'_q'])+pm[i][str(i)+'_qb']\n",
    "        k_matrix=np.matmul(x,pm[i][str(i)+'_k'])+pm[i][str(i)+'_kb']\n",
    "        v_matrix=np.matmul(x,pm[i][str(i)+'_v'])+pm[i][str(i)+'_vb']\n",
    "        #print(\"!\",q_matrix.shape, x.shape)\n",
    "        list_before_conca=list()\n",
    "        after_softmax_temp=[]\n",
    "        for j in range(self.batch_size):\n",
    "            before_softmax=np.matmul(q_matrix[j,:,:],k_matrix[j,:,:].transpose())\\\n",
    "            /np.sqrt(int(self.word_vector_dimension/self.num_heads))\n",
    "            #print(q_matrix[j,:,:].shape,k_matrix[j,:,:].shape)\n",
    "            if decoder ==1:\n",
    "                if self.predict==1:\n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        before_softmax[k,k+1:]=(-1e9)\n",
    "                    before_softmax[self.epoch_i+1:,self.epoch_i+1:]=(-1e9)\n",
    "                else:\n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        \n",
    "                        before_softmax[k,k+1:]=(-1e9)\n",
    "                    \n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        if self.input_mask_target[j,k]==0:\n",
    "                            \n",
    "                            before_softmax[:,k:]=(-1e9)\n",
    "                            \n",
    "                            break\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                for k in range(len(self.input_mask[0])):\n",
    "                    if self.input_mask[j,k]==1:\n",
    "                        before_softmax[:,k:]=(-1e9)\n",
    "        \n",
    "                        break\n",
    "                \n",
    "            #print(before_softmax)\n",
    "            after_softmax = softmax(before_softmax)\n",
    "            temp_result=np.matmul(after_softmax,v_matrix[j,:,:])\n",
    "            list_before_conca.append(temp_result)\n",
    "            after_softmax_temp.append(after_softmax)\n",
    "\n",
    "        aux_layers.append([x, q_matrix,k_matrix,before_softmax,\\\n",
    "                                      np.array(after_softmax_temp),v_matrix])\n",
    "        list_before_conca=np.array(list_before_conca)\n",
    "        \n",
    "        if i == 0:\n",
    "            final_attention_conca=list_before_conca\n",
    "        else:\n",
    "            final_attention_conca = np.concatenate((final_attention_conca,list_before_conca),axis=2)\n",
    "    \n",
    "\n",
    "    aux_layers.append(final_attention_conca)#1\n",
    "\n",
    "    final_attention_conca=np.matmul(final_attention_conca,pm[-1]['0_w'])+pm[-1]['0_wb']\n",
    "  \n",
    "    final_attention_conca_temp,temp=self.forward_batch_normal_layer(final_attention_conca+x,None,pm[-1]['0_batch']) #+x\n",
    "    aux_layers.append(temp) #6#3\n",
    "    aux_layers.append(final_attention_conca) #2\n",
    "    #atteintion_last\n",
    "    \n",
    "    if decoder ==1:\n",
    "        for i in range(self.num_heads):\n",
    "            q_matrix=np.matmul(final_attention_conca_temp,pm[i][str(i)+'_q2'])+pm[i][str(i)+'_q2b']\n",
    "            k_matrix=np.matmul(self.incoder_output,pm[i][str(i)+'_k2'])+pm[i][str(i)+'_k2b']\n",
    "            v_matrix=np.matmul(self.incoder_output,pm[i][str(i)+'_v2'])+pm[i][str(i)+'_v2b']\n",
    "            #print(k_matrix.shape)\n",
    "            list_before_conca=list()\n",
    "            after_softmax_temp=[]\n",
    "            for j in range(self.batch_size):\n",
    "                before_softmax=np.matmul(q_matrix[j,:,:],k_matrix[j,:,:].transpose())/np.sqrt(int(self.word_vector_dimension/self.num_heads))\n",
    "\n",
    "                after_softmax = softmax(before_softmax)\n",
    "\n",
    "                temp_result=np.matmul(after_softmax,v_matrix[j,:,:])\n",
    "                \n",
    "                list_before_conca.append(temp_result)\n",
    "                after_softmax_temp.append(after_softmax)\n",
    "            aux_layers.append([final_attention_conca_temp, q_matrix,k_matrix,np.sqrt(int(self.word_vector_dimension/self.num_heads)),\\\n",
    "                                      np.array(after_softmax_temp),v_matrix])\n",
    "            list_before_conca=np.array(list_before_conca)\n",
    "            \n",
    "\n",
    "            if i == 0:\n",
    "                final_attention_conca=list_before_conca\n",
    "            else:\n",
    "                final_attention_conca = np.concatenate((final_attention_conca,list_before_conca),axis=2)\n",
    "\n",
    "\n",
    "        aux_layers.append(final_attention_conca) #-7\n",
    "\n",
    "        final_attention_conca_1=np.matmul(final_attention_conca \\\n",
    "                    ,pm[-1]['3_w'])+pm[-1]['3_wb']\n",
    "\n",
    "        aux_layers.append(final_attention_conca_1) # -6\n",
    "        \n",
    "        final_attention_conca_temp,temp = self.forward_batch_normal_layer(final_attention_conca_1+final_attention_conca_temp,None,pm[-1]['1_batch'])# +final_attention_conca_temp\n",
    "        aux_layers.append(temp) # -5\n",
    "        aux_layers.append(final_attention_conca_temp) #-4\n",
    "\n",
    "    \n",
    "    final_attention_conca=relu(np.matmul(final_attention_conca_temp\\\n",
    "                    ,pm[-1]['1_w'])+pm[-1]['1_wb'])\n",
    "    aux_layers.append(final_attention_conca) #-3\n",
    "    #print(final_attention_conca_temp.shape,pm[-1]['1_w'].shape)\n",
    "    final_attention_conca=np.matmul(final_attention_conca\\\n",
    "                    ,pm[-1]['2_w'])+pm[-1]['2_wb']\n",
    "    aux_layers.append(final_attention_conca) #-2\n",
    "    final_attention_conca,temp = self.forward_batch_normal_layer(final_attention_conca+final_attention_conca_temp,None,pm[-1]['2_batch']) #+final_attention_conca_temp\n",
    "    aux_layers.append(temp) #-1\n",
    "\n",
    "    return final_attention_conca,aux_layers\n",
    "\n",
    "transformer.forward_multi_heads_layer=forward_multi_heads_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-3-0939931477c7>, line 319)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-0939931477c7>\"\u001b[1;36m, line \u001b[1;32m319\u001b[0m\n\u001b[1;33m    transformer.backprop_multi_heads_layer=backprop_multi_heads_layer\u001b[0m\n\u001b[1;37m                                                                     \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "def backprop_multi_heads_layer(self, G_input, pm, aux,decoder):\n",
    "\n",
    "    if pm is None: return G_y\n",
    "    #print(G_input)\n",
    "    #G_input=np.zeros_like(G_input)\n",
    "    \n",
    "    G_input = self.backprop_batch_normal_layer(G_input,None,pm[-1]['2_batch'],aux[-1])\n",
    "    \n",
    "    new_G_input=np.copy(G_input)                          \n",
    "    g_affine_input = pm[-1]['2_w'].transpose()\n",
    "    \n",
    "    for i in range(G_input.shape[0]):\n",
    "        if i ==0:\n",
    "            G_weight_=np.matmul(aux[-3][i].transpose(),G_input[i])\n",
    "        else:\n",
    "            G_weight_+=np.matmul(aux[-3][i].transpose(),G_input[i])\n",
    "    #print(g_affine_input.shape,G_input.shape,aux[-3][0].transpose().shape,G_weight_.shape)\n",
    "    \n",
    "    G_bias=np.sum(G_input, axis = (0,1))\n",
    "    self.update_param(pm[-1], '2_wb', G_bias)\n",
    "    self.update_param(pm[-1], '2_w', G_weight_)\n",
    "    \n",
    "    G_input = np.matmul(G_input, g_affine_input)\n",
    "    G_input = relu_derv(aux[-3])*G_input\n",
    "    \n",
    "    g_affine_input = pm[-1]['1_w'].transpose()\n",
    "    #print(G_input.shape, aux[-4].shape)\n",
    "    for i in range(G_input.shape[0]):\n",
    "        if i ==0:\n",
    "            G_weight_=np.matmul(aux[-4][i].transpose(),G_input[i])\n",
    "        else:\n",
    "            G_weight_+=np.matmul(aux[-4][i].transpose(),G_input[i])\n",
    "    G_bias=np.sum(G_input, axis = (0,1))\n",
    "    self.update_param(pm[-1], '1_wb', G_bias)\n",
    "    self.update_param(pm[-1], '1_w', G_weight_)\n",
    "    G_input_second = np.matmul(G_input, g_affine_input)+new_G_input\n",
    "    \n",
    "    #print(G_input_second)\n",
    "    if decoder ==1:\n",
    "                \n",
    "        G_input_second = self.backprop_batch_normal_layer(G_input_second,None,pm[-1]['1_batch'],aux[-5])\n",
    "        new_G_input=np.copy(G_input_second)\n",
    "        #print(\"input\",G_input_second.shape)\n",
    "        g_affine_input = pm[-1]['3_w'].transpose()\n",
    "        for i in range(G_input_second.shape[0]):\n",
    "            if i ==0:\n",
    "                G_weight_=np.matmul(aux[-7][i].transpose(),G_input_second[i])\n",
    "            else:\n",
    "                G_weight_+=np.matmul(aux[-7][i].transpose(),G_input_second[i])\n",
    "        \n",
    "        G_bias=np.sum(G_input_second, axis = (0,1))\n",
    "        self.update_param(pm[-1], '3_wb', G_bias)\n",
    "        self.update_param(pm[-1], '3_w', G_weight_) #?????????????????????????????????????\n",
    "        \n",
    "        G_input_ = np.matmul(G_input_second, g_affine_input)\n",
    "        \n",
    "        G_input_second=np.zeros_like(G_input_)\n",
    "        for i in range(self.num_heads):\n",
    "            G_input_temp=G_input_[:,:,(G_input_.shape[-1]/self.num_heads)*i:(G_input_.shape[-1]/self.num_heads)*(i+1)]\n",
    "            #print(\"중간\",G_input_temp.shape)\n",
    "            aux_temp_num=aux[-8+(i-self.num_heads+1)]\n",
    "            \n",
    "            \n",
    "            for j in range(self.batch_size):\n",
    "                if j == 0:\n",
    "\n",
    "                    g_affine_weight = aux_temp_num[-2][j,:,:].transpose()\n",
    "                    g_affine_input = aux_temp_num[-1][j,:,:].transpose()\n",
    "                    G_v = np.matmul(g_affine_weight, G_input_temp[j,:,:])\n",
    "                    G_input = np.matmul(G_input_temp[j,:,:], g_affine_input)\n",
    "                    G_input = G_input*softmax_derv(aux_temp_num[-2][j,:,:])\n",
    "                    G_input=G_input/self.root_\n",
    "                    g_affine_weight = aux_temp_num[1][j,:,:].transpose()\n",
    "                    g_affine_input = aux_temp_num[2][j,:,:]\n",
    "                    G_k = np.matmul(g_affine_weight, G_input).transpose()\n",
    "                    G_q = np.matmul(G_input, g_affine_input)\n",
    "                    G_v = G_v[np.newaxis,:,:]\n",
    "                    G_k = G_k[np.newaxis,:,:]\n",
    "                    G_q = G_q[np.newaxis,:,:]\n",
    "                    \n",
    "                    #print(\"back\",G_k.shape,G_q.shape)\n",
    "                else:\n",
    "                    g_affine_weight = aux_temp_num[-2][j,:,:].transpose()\n",
    "                    g_affine_input = aux_temp_num[-1][j,:,:].transpose()\n",
    "                    G_v_temp=np.matmul(g_affine_weight, G_input_temp[j,:,:])\n",
    "                    G_input = np.matmul(G_input_temp[j,:,:], g_affine_input)\n",
    "                    G_input = G_input*softmax_derv(aux_temp_num[-2][j,:,:])\n",
    "                    G_input=G_input/self.root_\n",
    "                    g_affine_weight = aux_temp_num[1][j,:,:].transpose()\n",
    "                    g_affine_input = aux_temp_num[2][j,:,:]\n",
    "                    G_k_temp = np.matmul(g_affine_weight, G_input).transpose()\n",
    "                    \n",
    "                    G_q_temp = np.matmul(G_input, g_affine_input)\n",
    "                    G_v_temp=G_v_temp[np.newaxis,:,:]\n",
    "                    G_k_temp=G_k_temp[np.newaxis,:,:]\n",
    "                    G_q_temp=G_q_temp[np.newaxis,:,:]\n",
    "                    G_q = np.concatenate((G_q, G_q_temp),axis=0)\n",
    "                    G_k = np.concatenate((G_k, G_k_temp),axis=0)\n",
    "                    G_v = np.concatenate((G_v, G_v_temp),axis=0)\n",
    "            #print(\"중이후\",G_q.shape)\n",
    "                    \n",
    "            G_bias=np.sum(G_q, axis = (0,1))\n",
    "            self.update_param(pm[i], str(i)+'_q2b', G_bias)\n",
    "            G_bias=np.sum(G_k, axis = (0,1))\n",
    "            self.update_param(pm[i], str(i)+'_k2b', G_bias)\n",
    "            G_bias=np.sum(G_v, axis = (0,1))\n",
    "            #print(G_bias)\n",
    "            self.update_param(pm[i], str(i)+'_v2b', G_bias)\n",
    "            \n",
    "            for j in range(self.batch_size):\n",
    "                g_affine_weight = self.incoder_output[j].transpose()\n",
    "                if j == 0:\n",
    "                    G_weight_4 = np.matmul(g_affine_weight, G_k[j,:,:])\n",
    "                else:\n",
    "                    G_weight_4 += np.matmul(g_affine_weight, G_k[j,:,:])\n",
    "\n",
    "                g_affine_weight = self.incoder_output[j].transpose()\n",
    "\n",
    "                if j == 0:\n",
    "                    G_weight_5= np.matmul(g_affine_weight, G_v[j,:,:])\n",
    "                else:\n",
    "                    G_weight_5+= np.matmul(g_affine_weight, G_v[j,:,:])\n",
    "                    \n",
    "                g_affine_weight = aux_temp_num[0][j].transpose()\n",
    "\n",
    "                if j == 0:\n",
    "                    G_weight_6 = np.matmul(g_affine_weight, G_q[j,:,:])\n",
    "                else:\n",
    "                    G_weight_6 += np.matmul(g_affine_weight, G_q[j,:,:])\n",
    "                    \n",
    "            if self.first_time == 1:\n",
    "                self.G_input_encoder=np.zeros_like(self.incoder_output)\n",
    "                self.first_time=0\n",
    "            \n",
    "            g_affine_input = pm[i][str(i)+'_k2'].transpose()\n",
    "            #print(self.G_input_encoder.shape,np.matmul(G_k, g_affine_input).shape)\n",
    "            #print(G_k.shape, g_affine_input.shape)\n",
    "            self.G_input_encoder+=np.matmul(G_k, g_affine_input)\n",
    "\n",
    "            g_affine_input = pm[i][str(i)+'_v2'].transpose()\n",
    "            self.G_input_encoder += np.matmul(G_v, g_affine_input)\n",
    "            g_affine_input = pm[i][str(i)+'_q2'].transpose()\n",
    "            #print(G_q.shape, g_affine_input.shape,np.matmul(G_q, g_affine_input).shape)\n",
    "            G_input_second += np.matmul(G_q, g_affine_input)\n",
    "            \n",
    "            self.update_param(pm[i], str(i)+'_k2', G_weight_4)\n",
    "            self.update_param(pm[i], str(i)+'_v2', G_weight_5)\n",
    "            self.update_param(pm[i], str(i)+'_q2', G_weight_6)\n",
    "            #print(\"축소\",G_input_second.shape)\n",
    "            \n",
    "        G_input_second += new_G_input\n",
    "    #print(G_input_second)\n",
    "    #confirm.\n",
    "       \n",
    "    G_input_second = self.backprop_batch_normal_layer(G_input_second,None,pm[-1]['0_batch'],aux[self.num_heads+1])\n",
    "    \n",
    "    new_G_input=np.copy(G_input_second)\n",
    "    \n",
    "    \n",
    "    g_affine_input = pm[-1]['0_w'].transpose()\n",
    "\n",
    "    for i in range( G_input_second.shape[0]):\n",
    "        if i ==0:\n",
    "            G_weight_=np.matmul(aux[self.num_heads][i].transpose(),G_input_second[i])\n",
    "        else:\n",
    "            G_weight_+=np.matmul(aux[self.num_heads][i].transpose(),G_input_second[i])\n",
    "            \n",
    "    G_bias=np.sum(G_input_second, axis = (0,1))\n",
    "    self.update_param(pm[-1], '0_wb', G_bias)\n",
    "    self.update_param(pm[-1], '0_w', G_weight_)\n",
    "    #print(G_bias)\n",
    "    \n",
    "    G_input_final=np.zeros_like(G_input_second)\n",
    "    G_input_ = np.matmul(G_input_second, g_affine_input)\n",
    "    #print(G_input_)\n",
    "    for i in range(self.num_heads):\n",
    "\n",
    "        G_input_temp=G_input_[:,:,(G_input_.shape[-1]/self.num_heads)*i:(G_input_.shape[-1]/self.num_heads)*(i+1)]\n",
    "        aux_temp_num=aux[i]\n",
    "        for j in range(self.batch_size):\n",
    "            if j == 0:\n",
    "                g_affine_weight = aux_temp_num[-2][j,:,:].transpose()\n",
    "                g_affine_input = aux_temp_num[-1][j,:,:].transpose()\n",
    "                G_v = np.matmul(g_affine_weight, G_input_temp[j,:,:])\n",
    "                G_input = np.matmul(G_input_temp[j,:,:], g_affine_input)\n",
    "                G_input = G_input*softmax_derv(aux_temp_num[-2][j,:,:])\n",
    "                #print(G_input)\n",
    "                #print(G_input)\n",
    "                if decoder ==1:\n",
    "\n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        G_input[k,k+1:]=0\n",
    "                    #print(G_input)\n",
    "                    \n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        if self.input_mask_target[j,k]==0:\n",
    "                            G_input[:,k:]=0\n",
    "                            break\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    \n",
    "                    for k in range(len(self.input_mask[0])):\n",
    "                        if self.input_mask[j,k]==1:\n",
    "                            G_input[:,k:]=0\n",
    "                            break\n",
    "                    \n",
    "                G_input=G_input/self.root_\n",
    "                g_affine_weight = aux_temp_num[1][j,:,:].transpose()\n",
    "                g_affine_input = aux_temp_num[2][j,:,:]\n",
    "                G_k = np.matmul(g_affine_weight, G_input).transpose()\n",
    "                G_q = np.matmul(G_input, g_affine_input)\n",
    "         \n",
    "                G_v = G_v[np.newaxis,:,:]\n",
    "                G_k = G_k[np.newaxis,:,:]\n",
    "                G_q = G_q[np.newaxis,:,:]\n",
    "    \n",
    "            else:\n",
    "                g_affine_weight = aux_temp_num[-2][j,:,:].transpose()\n",
    "                g_affine_input = aux_temp_num[-1][j,:,:].transpose()\n",
    "                G_v_temp=np.matmul(g_affine_weight, G_input_temp[j,:,:])\n",
    "                G_v_temp=G_v_temp[np.newaxis,:,:]\n",
    "                G_input = np.matmul(G_input_temp[j,:,:], g_affine_input)\n",
    "                G_input = G_input*softmax_derv(aux_temp_num[-2][j,:,:])\n",
    "                if decoder ==1:\n",
    "\n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        G_input[k,k+1:]=0\n",
    "                    #print(G_input)\n",
    "                    \n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        if self.input_mask_target[j,k]==0:\n",
    "                            G_input[:,k:]=0\n",
    "                            break\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    \n",
    "                    for k in range(len(self.input_mask[0])):\n",
    "                        if self.input_mask[j,k]==1:\n",
    "                            G_input[:,k:]=0\n",
    "                            break\n",
    "                    \n",
    "                #print(G_input)\n",
    "                G_input=G_input/self.root_\n",
    "                \n",
    "                g_affine_weight = aux_temp_num[1][j,:,:].transpose()\n",
    "                g_affine_input = aux_temp_num[2][j,:,:]\n",
    "                G_k_temp = np.matmul(g_affine_weight, G_input).transpose()\n",
    "                G_k_temp=G_k_temp[np.newaxis,:,:]\n",
    "                G_q_temp = np.matmul(G_input, g_affine_input)\n",
    "                G_q_temp=G_q_temp[np.newaxis,:,:]\n",
    "                G_q = np.concatenate((G_q, G_q_temp),axis=0)\n",
    "                G_v = np.concatenate((G_v, G_v_temp),axis=0)\n",
    "                G_k = np.concatenate((G_k, G_k_temp),axis=0)\n",
    "     \n",
    "    \n",
    "        \n",
    "        for j in range(self.batch_size):\n",
    "            g_affine_weight = aux_temp_num[0][j].transpose()\n",
    "   \n",
    "            if j==0:\n",
    "                G_weight_8 = np.matmul(g_affine_weight, G_k[j,:,:])\n",
    "            else:\n",
    "                G_weight_8 += np.matmul(g_affine_weight, G_k[j,:,:])\n",
    "\n",
    "            if j==0:\n",
    "                G_weight_9 = np.matmul(g_affine_weight, G_v[j,:,:])\n",
    "            else:\n",
    "                G_weight_9 += np.matmul(g_affine_weight, G_v[j,:,:])\n",
    " \n",
    "            if j==0:\n",
    "                G_weight_10= np.matmul(g_affine_weight, G_q[j,:,:])\n",
    "            else:\n",
    "                G_weight_10+= np.matmul(g_affine_weight, G_q[j,:,:])\n",
    "   \n",
    "        #print(G_q)\n",
    "    \n",
    "        \n",
    "        G_bias=np.sum(G_q, axis = (0,1))\n",
    "        #print(G_bias)\n",
    "        self.update_param(pm[i], str(i)+'_qb', G_bias)\n",
    "        \n",
    "        G_bias=np.sum(G_k, axis = (0,1))\n",
    "        self.update_param(pm[i], str(i)+'_kb', G_bias)\n",
    "        G_bias=np.sum(G_v, axis = (0,1))\n",
    "        self.update_param(pm[i], str(i)+'_vb', G_bias)\n",
    "        \n",
    "        \n",
    "\n",
    "        g_affine_input = pm[i][str(i)+'_k'].transpose()\n",
    "        G_input_final += np.matmul(G_k, g_affine_input)\n",
    "        g_affine_input = pm[i][str(i)+'_v'].transpose()\n",
    "        G_input_final += np.matmul(G_v, g_affine_input)\n",
    "        g_affine_input = pm[i][str(i)+'_q'].transpose()\n",
    "        G_input_final += np.matmul(G_q, g_affine_input)\n",
    "        self.update_param(pm[i], str(i)+'_k', G_weight_8)\n",
    "        self.update_param(pm[i], str(i)+'_q', G_weight_10)\n",
    "        self.update_param(pm[i], str(i)+'_v', G_weight_9)\n",
    "    G_input_final += new_G_input\n",
    "    \n",
    "    return G_input_final\n",
    "    \n",
    "\n",
    "transformer.backprop_multi_heads_layer=backprop_multi_heads_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c94cc21fca8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_translation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_translation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "def test(self,a='is', b='are'):\n",
    "    a_=self.text_dataset.word_to_id[a]\n",
    "    b_=self.text_dataset.word_to_id[b]\n",
    "\n",
    "    print(np.dot(self.pm_hiddens[0]['dic'][a_], self.pm_hiddens[0]['dic'][b_])/((sum(self.pm_hiddens[0]['dic'][a_]*self.pm_hiddens[0]['dic'][a_])\\\n",
    "                                                                                **0.5)*(sum(self.pm_hiddens[0]['dic'][b_]*self.pm_hiddens[0]['dic'][b_])**0.5)))\n",
    "\n",
    "def predict_translation(self,sentence_1,t):\n",
    "\n",
    "    result_list=list()\n",
    "    temp_list=list()\n",
    "    sentence_2=self.predict_forward_neuralnet(sentence_1,t)\n",
    "\n",
    "    for i in range(self.batch_size):\n",
    "        temp_list_list=list()\n",
    "        result_list_list=list()\n",
    "        for w in sentence_1.tolist()[i]:\n",
    "            if self.text_dataset.original_unique_words==w:\n",
    "                pass\n",
    "            else:\n",
    "                temp_list_list.append(self.text_dataset.original_id_to_word[w])\n",
    "        temp_list.append(temp_list_list)\n",
    "        \n",
    "        for w in sentence_2.tolist()[i]:\n",
    "            result_list_list.append(self.text_dataset.target_id_to_word[w])\n",
    "        result_list.append(result_list_list)\n",
    "\n",
    "        print(temp_list[i],\"->\",result_list[i])\n",
    "        print(\" \")\n",
    "\n",
    "\n",
    "transformer.test=test\n",
    "transformer.predict_translation=predict_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alloc_embedding_layer(self):\n",
    "    input_cnt = len(self.text_dataset.original_id_to_word)\n",
    "    output_cnt = self.word_vector_dimension\n",
    "    a = transformer.making_weight_em(self,input_cnt, output_cnt)\n",
    "    self.G_original_embedding_like=np.zeros_like(a)\n",
    "    return {'dic':a}\n",
    "\n",
    "def forward_embedding_layer(self, x, hconfig, pm):\n",
    "    output=pm['dic'][x,:]+self.sin_cos_\n",
    "    self.input_mask=np.sign(x-len(self.text_dataset.original_id_to_word))+1\n",
    "    \n",
    "    return output, x\n",
    "\n",
    "def backprop_embedding_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "    x= aux\n",
    "\n",
    "    for i in range(self.batch_size):\n",
    "        self.G_original_embedding_like[x[i],:]+=G_y[i]\n",
    "    self.update_param(pm, 'dic', self.G_original_embedding_like)\n",
    "    self.G_original_embedding_like=np.zeros_like(self.G_original_embedding_like)\n",
    "    self.first_time=1\n",
    "    return 0\n",
    "transformer.alloc_embedding_layer=alloc_embedding_layer\n",
    "transformer.forward_embedding_layer =forward_embedding_layer\n",
    "transformer.backprop_embedding_layer=backprop_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alloc_target_embedding_layer(self):\n",
    "    input_cnt = len(self.text_dataset.target_id_to_word)\n",
    "    output_cnt = self.word_vector_dimension\n",
    "    a = transformer.making_weight_em(self,input_cnt, output_cnt)\n",
    "    self.G_target_embedding_like=np.zeros_like(a)\n",
    "    return {'dic':a}\n",
    "\n",
    "def forward_target_embedding_layer(self, x, hconfig, pm):\n",
    "    output=pm['dic'][x,:]+self.sin_cos_target\n",
    "    self.input_mask_target=np.sign(x-len(self.text_dataset.target_id_to_word)+2)\n",
    "\n",
    "    return output, x\n",
    "\n",
    "def backprop_target_embedding_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "    x= aux\n",
    "\n",
    "    for i in range(self.batch_size):\n",
    "        self.G_target_embedding_like[x[i],:]=G_y[i]\n",
    "    self.update_param(pm, 'dic', self.G_target_embedding_like)\n",
    "    self.G_target_embedding_like=np.zeros_like(self.G_target_embedding_like)\n",
    "    \n",
    "    return 0\n",
    "transformer.alloc_target_embedding_layer=alloc_target_embedding_layer\n",
    "transformer.forward_target_embedding_layer =forward_target_embedding_layer\n",
    "transformer.backprop_target_embedding_layer=backprop_target_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoder(self):\n",
    "    denominator=np.power(10000,2*np.tile(np.arange(self.word_vector_dimension, dtype='float64').reshape((1,-1)),(self.text_dataset.most_length,1))/(self.word_vector_dimension))\n",
    "    numerator=np.tile(np.arange(self.text_dataset.most_length, dtype='float64').reshape((-1,1)),self.word_vector_dimension)\n",
    "    positional_encoding_matrix=numerator/denominator\n",
    "    sin_=np.sin(positional_encoding_matrix)\n",
    "    cos_=np.cos(positional_encoding_matrix)\n",
    "    sin_cos_=np.zeros_like(sin_)\n",
    "    sin_cos_[:,0::2]=sin_[:,0::2]\n",
    "    sin_cos_[:,1::2]=cos_[:,1::2]\n",
    "    self.sin_cos_=sin_cos_\n",
    "\n",
    "def position_decoder(self):\n",
    "    denominator=np.power(10000,2*np.tile(np.arange(self.word_vector_dimension, dtype='float64').reshape((1,-1))\\\n",
    "                                         ,(self.text_dataset.most_length_target,1))/(self.word_vector_dimension))\n",
    "    numerator=np.tile(np.arange(self.text_dataset.most_length_target, dtype='float64').reshape((-1,1)),self.word_vector_dimension)\n",
    "    positional_encoding_matrix=numerator/denominator\n",
    "    sin_=np.sin(positional_encoding_matrix)\n",
    "    cos_=np.cos(positional_encoding_matrix)\n",
    "    sin_cos_=np.zeros_like(sin_)\n",
    "    sin_cos_[:,0::2]=sin_[:,0::2]\n",
    "    sin_cos_[:,1::2]=cos_[:,1::2]\n",
    "    self.sin_cos_target=sin_cos_\n",
    "    \n",
    "transformer.position_encoder=position_encoder\n",
    "transformer.position_decoder=position_decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
